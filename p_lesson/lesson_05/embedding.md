### 什么是Embedding

### 向量数据库

### 下载地址https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Seattle_Hotels.csv
* 字段： name， address， desc

### 为酒店建立内容推荐系统
#### 余弦相似度：
#### 余弦相似度是一种常用的相似度度量方法，用于衡量两个向量之间的相似性
#### 它通过计算两个向量之间的余弦值来度量他们之间的相似性
#### 两个向量之间夹角的余弦值为(-1, 1)


#### 什么是N-Gram(N元语法)
#### N-Gram是指将文本序列中的连续N个词或字符作为一个单位进行处理的技术
#### 例如，对于句子"我喜欢自然语言处理"，2-gram(即N=2)将被表示为["我喜欢", "喜欢自然", "自然语言", "语言处理"]
#### N-Gram在自然语言处理中被广泛应用，例如文本分类、情感分析、机器翻译等任务中

#### 为什么需要用到2元语法，3元语法，N元语法？
#### 2元语法(2-gram)：
#### 2-gram可以捕捉到文本中的局部信息，例如相邻的两个词之间的关系。
#### 在文本分类任务中，2-gram可以帮助模型识别出哪些特征组合更能代表某个类别。
#### 3元语法(3-gram)：
#### 3-gram可以捕捉到文本中的更复杂的关系，例如三个相邻的词之间的关系。
#### 在情感分析任务中，3-gram可以帮助模型识别出哪些特征组合更能代表某个情感。
#### N元语法(N-gram)：
#### N-gram可以捕捉到文本中的更复杂的关系，例如N个相邻的词或字符之间的关系。
#### 在机器翻译任务中，N-gram可以帮助模型识别出哪些特征组合更能代表某个翻译。
#### 一般采用1-4元语法，根据任务的不同，选择合适的N值。
#### 4元语法的话，3元语法也要算，2元语法也要算，1元语法也要算

#### 停用词是什么？
#### 停用词是指在文本处理中被频繁出现但对文本语义没有实质贡献的词
#### 例如，"的"、"了"、"是"、"在"等词在中文文本中被广泛使用，但对文本的语义分析没有实质帮助
#### 停用词通常会被过滤掉，以减少文本数据的维度和计算量
#### 在自然语言处理任务中，停用词的处理是一个重要的预处理步骤，有助于提高模型的性能和效率

### 停用词的处理
#### 停用词的处理通常包括两个步骤：
#### 1. 停用词列表的构建：
#### 停用词列表可以根据具体任务和领域进行构建。
#### 例如，在中文文本分类任务中，常用的停用词包括"的"、"了"、"是"、"在"等。
#### 2. 停用词的过滤：
#### 在文本处理过程中，将停用词从文本中过滤掉是一个重要的步骤。
#### 可以使用简单的字符串匹配或正则表达式来识别和过滤停用词。
#### 例如，在Python中可以使用nltk库来处理停用词，代码如下：

```
from nltk.corpus import stopwords
stop_words = set(stopwords.words('chinese'))
text = "我喜欢自然语言处理"
words = text.split()
filtered_words = [word for word in words if word not in stop_words]
print(filtered_words)
```

#### 输出结果为：["我喜欢", "自然语言", "处理"]

#### 使用TF-IDF(词频-逆文档频率)
#### TF-IDF是一种用于文本特征提取的技术，用于评估一个词在文档集合中的重要性
#### 它由两个部分组成：词频(TF)和逆文档频率(IDF)
#### 词频(TF)：
#### 词频表示一个词在文档中出现的次数。
#### 假设一个文档包含N个词，词w在文档中出现的次数为T(w)，则词w的词频为TF(w) = T(w) / N
#### 逆文档频率(IDF)：
#### 逆文档频率表示一个词在文档集合中出现的频率。
#### 假设文档集合包含M个文档，其中包含词w的文档数量为D(w)，则词w的逆文档频率为IDF(w) = log(M / D(w))
#### TF-IDF值：
#### 一个词的TF-IDF值等于它的词频乘以逆文档频率，即TF-IDF(w) = TF(w) * IDF(w)
#### TF-IDF值越大，表示一个词在文档集合中的重要性越高

#### 向量表示
#### 在文本处理中，将文本转换为向量表示是一个重要的步骤。
#### 常用的向量表示方法包括Bag-of-Words(BoW)模型和TF-IDF模型。
#### BoW模型将文本表示为一个固定长度的向量，每个元素表示一个词在文本中出现的次数。
#### TF-IDF模型则将文本表示为一个固定长度的向量，每个元素表示一个词在文本中的重要性。

#### 拿到TF-IDF向量后，就可以使用余弦相似度等方法来计算文本之间的相似度。

#### TF-IDF 
* TF： Term Frequency 词频
* IDF： Inverse Document Frequency 逆文档频率 可以理解为区分度。这个单词出现的文档数越少，区分度就越高，IDF值就越大。

#### 基于内容的推荐原理
1. 对描述进行特征提取
  * N-Gram，提取N个连续的词或字符作为一个特征
  * 停用词过滤，移除停用词
  * 词干提取，将词转换为其基本形式
  * TF-IDF，按照(min_df, max_df)提取关键词，并生成TF-IDF矩阵
2. 计算相似度
  * 对用户输入的描述进行特征提取，生成TF-IDF向量
  * 计算用户输入向量与每个酒店描述向量的余弦相似度
  * 根据相似度排序，推荐相似度最高的酒店
3. 对于指定用户，推荐相似度最高的酒店

#### 什么是Embedding
#### Embedding是一种将高维空间中的向量映射到低维空间中的技术，用于表示文本、图像、音频等数据的特征。
#### 它的目标是将原始数据映射到一个连续的向量空间中，使得相似的数据点在向量空间中距离更近。
#### 常用的Embedding模型包括Word2Vec、GloVe、FastText等。
#### 这些模型通过训练神经网络模型，将文本中的每个词映射到一个低维向量空间中，从而实现了对文本的特征提取和表示。

#### Embedding is all you need
* 万事万物都可以用Embedding表示，
* 维度：Embedding向量的维度通常是一个固定的数值，例如50、100、200等。
* 不同维度的Embedding向量可以表示不同程度的语义信息，维度越高，语义信息越丰富。
* 例如，50维的Embedding向量可以表示50个不同的特征，而200维的Embedding向量可以表示200个不同的特征。
* 向量还可以进行运算
* king - man + woman ~= queen
* 这个公式的意思是，king和man的向量表示减去man的向量表示，再加上woman的向量表示，得到的结果向量与queen的向量表示最为接近。
* 这个公式在Embedding模型中被广泛应用，用于表示词之间的关系。


### Word Embedding
#### Word2Vec
#### Word2Vec是一种常用的Embedding模型，用于将文本中的每个词映射到一个低维向量空间中。
#### 它的基本思想是通过训练一个神经网络模型，将文本中的每个词转换为一个固定长度的向量表示。
#### 这些向量表示可以捕捉到词之间的语义关系，例如，"king"和"queen"的向量表示在向量空间中距离更近。
#### Word2Vec模型有两种主要的实现方式：CBOW(Continuous Bag-of-Words)和Skip-gram。
#### CBOW模型通过预测当前词基于上下文词的出现概率，来学习词的向量表示。
 * 例如，假设上下文词为"the", "quick", "brown"，当前词为"fox"，则CBOW模型的目标是预测"fox"基于"the", "quick", "brown"的出现概率。
#### Skip-gram模型则是通过预测上下文词基于当前词的出现概率，来学习词的向量表示。
  * 例如，假设当前词为"fox"，上下文词为"the", "quick", "brown"，则Skip-gram模型的目标是预测"the", "quick", "brown"基于"fox"的出现概率。
#### 这两种模型都可以通过大规模的文本数据进行训练，从而学习到丰富的词向量表示。
#### 原理： 这两种模型都基于神经网络模型，通过训练模型来学习词的向量表示。
#### 具体来说，CBOW模型通过预测当前词基于上下文词的出现概率，来学习词的向量表示。
#### Skip-gram模型则是通过预测上下文词基于当前词的出现概率，来学习词的向量表示。
#### 这两种模型都可以通过大规模的文本数据进行训练，从而学习到丰富的词向量表示。


#### 隐藏层 -- 没明白，先标记上(b站10min左右)
#### 这两种模型都包含一个隐藏层，用于学习词的向量表示。
#### 隐藏层的输出向量就是词的Embedding向量。
#### 例如，假设我们有一个包含10000个词的词汇表，每个词用一个50维的向量表示，那么我们可以将隐藏层的输出表示为一个10000x50的矩阵，其中每一行就是一个词的Embedding向量。

#### 自监督学习
#### 自监督学习是一种无监督学习方法，它通过在没有人工标注的情况下，从数据中学习到有用的特征表示。
#### 自监督学习的基本思想是，通过设计一个 pretext task( pretext任务)，将原始数据转换为一个新的表示形式，从而学习到数据的潜在结构。
#### 例如，在图像分类任务中，我们可以将图像中的像素随机mask掉，然后通过预测被mask掉的像素值来学习到图像的特征表示。
#### 自监督学习在自然语言处理(NLP)领域中得到了广泛的应用，例如，在Word2Vec模型中，我们可以使用CBOW模型来预测当前词基于上下文词的出现概率，从而学习到词的向量表示。
#### 自监督学习的优势在于，它不需要人工标注的数据，而是通过从数据中学习到有用的特征表示，从而实现了无监督学习的目标。

#### 一些疑问
1. input是什么 output是什么，人为设置的吗？
2. 隐藏层是什么?
3. 多少维是什么意思？
  * 多少维指的是Embedding向量的维度，通常是一个固定的数值，例如50、100、200等。
  * 不同维度的Embedding向量可以表示不同程度的语义信息，维度越高，语义信息越丰富。
  * 例如，50维的Embedding向量可以表示50个不同的特征，而200维的Embedding向量可以表示200个不同的特征。
  * 3.1 多少维可以理解为特征的个数吗？
    * 是的，多少维可以理解为特征的个数。
    * 例如，50维的Embedding向量可以表示50个不同的特征，每个特征对应一个维度。
    * 200维的Embedding向量可以表示200个不同的特征，每个特征对应一个维度。
    * 因此，多少维可以理解为特征的个数，也可以理解为Embedding向量的维度。
4. 初始的向量是随机的吗？
  * 是的，初始的向量是随机的。
  * 例如，假设我们有一个包含10000个词的词汇表，每个词用一个50维的向量表示，那么我们可以将隐藏层的输出表示为一个10000x50的矩阵，其中每一行就是一个词的Embedding向量。
  * 这些向量是随机初始化的，然后通过训练模型来学习到有用的特征表示。
  * 整个神经网络的训练，会经过多个epoch，每个epoch会对所有的训练样本进行一次前向传播和反向传播。
  * 在每个epoch中，模型会根据损失函数来更新参数，从而学习到更好的特征表示。
  * 整个训练过程中，模型会不断优化参数，直到损失函数收敛，或者达到预设的训练次数。

#### jieba
#### jieba是一个用于中文分词的Python库，它可以将中文文本分割为单独的词语。
#### 我们可以使用jieba来对中文文本进行分词，从而为后续的自然语言处理任务提供基础。
#### 例如，假设我们有一个包含中文文本的文件"text.txt"，我们可以使用以下代码来对其进行分词：
```python
import jieba

# 读取文本数据文件
with open("text.txt", "r") as f:
    lines = f.readlines()

# 对每一行文本进行分词
for line in lines:
    seg_list = jieba.cut(line)
    print(" ".join(seg_list))
```



#### Gensim
#### Gensim是一个用于自然语言处理(NLP)的Python库，它提供了一些常用的NLP工具，例如Word2Vec模型。
#### 我们可以使用Gensim来训练Word2Vec模型，从而学习到词的向量表示。
#### 例如，假设我们有一个包含10000个词的词汇表，每个词用一个50维的向量表示，那么我们可以使用Gensim来训练一个Word2Vec模型，从而学习到每个词的Embedding向量。
#### Gensim的使用非常简单，我们只需要提供一个包含文本数据的文件，然后调用Gensim的Word2Vec函数即可。
#### 例如，假设我们的文本数据文件为"text.txt"，我们可以使用以下代码来训练一个Word2Vec模型：
```python
from genism.models import Word2Vec
import multiprocessing

sentences = word2vec.PathLineSentences("text.txt")
# 设置模型参数，进行训练
model = Word2Vec(sentences, size=100, window=3, min_count=1)
print(model.wv.similarity("孙悟空", "孙行者"))
print(model.wv.similarity("孙悟空", "猪八戒"))
# 设置模型参数，进行训练
model2 = Word2Vec(sentences, size=128, window=5, min_count=5)
workers = multiprocessing.cpu_count()
model2.train(sentences, total_examples=model2.corpus_count, epochs=model2.epochs, workers=workers)
print(model2.wv.similarity("孙悟空", "孙行者"))
print(model2.wv.similarity("孙悟空", "猪八戒"))

# 保存模型
model.save("model1.model")
model2.save("model2.model")
# 加减法
#...

```

#### 推理大模型
* 我们给他prompt，他给我们response
#### 语义理解模型(嵌入模型) Embedding模型 
* 帮我们找相似的词
* 帮我们找相似的句子
* 帮我们理解句子的意思
* 帮我们找相似的片段
##### Embedding模型选择
* BGE-M3
...
#### 视觉大模型
* dino

### 练习题
* 使用Gensim中的Word2Vec对三国演义进行Word Embedding，分析和曹操最相近的词有哪些，
* 曹操+刘备-张飞=?

### 向量数据库
#### 什么是向量数据库
* 向量数据库是一种特殊的数据库，它用于存储和管理向量数据。
  * 可以理解为存储特征码？？
  * 每个向量都有一个唯一的ID，我们可以根据ID来查询向量数据。
  * 向量数据库可以用于存储和管理大规模的向量数据，例如文本的Embedding向量、图像的特征向量等。
* 向量数据是指由多个数值组成的向量，例如文本的Embedding向量、图像的特征向量等。
* 向量数据库的主要作用是快速地进行向量相似度查询，例如找到和给定向量最相似的其他向量。
* 向量数据库通常使用特殊的索引结构，例如倒排索引、树结构等，来加速相似度查询。
* 向量数据库在自然语言处理、计算机视觉等领域有广泛的应用，例如推荐系统、图像检索等。

#### 有哪些向量数据库
* FAISS
  * 特点
    * 基于倒排索引和树结构的索引结构，支持高维向量的相似度查询。
    * 支持批量查询和实时查询。
    * 支持GPU加速。
* Milvus
  * 特点
    * 基于云原生架构，支持水平扩展和高可用性。
    * 支持多种索引结构，例如IVF_FLAT、IVF_PQ、HNSW等。
    * 支持批量插入和实时查询。
* Pinecone
  * 特点
    * 基于云原生架构，支持水平扩展和高可用性。
    * 支持多种索引结构，例如IVF_FLAT、IVF_PQ、HNSW等。
    * 支持批量插入和实时查询。
* Chroma
* Qdrant
* Weaviate

#### 向量数据库 vs 传统数据库
* 数据类型
  * 向量数据库主要用于存储和管理向量数据，例如文本的Embedding向量、图像的特征向量等。
  * 传统数据库主要用于存储和管理结构化数据，例如关系型数据库中的表。
* 查询方式
  * 向量数据库通常使用特殊的索引结构，例如倒排索引、树结构等，来加速相似度查询。
  * 传统数据库通常使用关系型查询语言(如SQL)来查询数据。
* 应用场景
  * 向量数据库通常用于需要快速进行向量相似度查询的场景，例如推荐系统、图像检索等。
  * 传统数据库通常用于需要结构化数据查询的场景，例如关系型数据库中的表。

#### 向量数据库一般最开始就选型，根据应用场景和性能需求来选择合适的向量数据库。

#### XX故障如何处理
1. 如果知道故障的代码 -> 结构化数据，很精确的查找
2. 向量数据库 -> 找回多个相似的信息，例如相似的文本、相似的图像等。

#### RAG
* RAG是Retrieval-Augmented Generation的缩写，它是一种基于检索的生成模型。
* RAG的基本思想是先从一个大的文本 corpus 中检索出与输入相关的信息，然后将这些信息作为上下文，与输入一起输入到一个生成模型中，生成符合上下文的输出。
* RAG的优势在于它可以利用外部的知识库来增强生成模型的能力，从而生成更符合上下文的输出。
* RAG在自然语言处理、计算机视觉等领域有广泛的应用，例如推荐系统、图像检索等。





















