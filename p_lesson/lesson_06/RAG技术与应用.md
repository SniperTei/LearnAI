## RAG技术与应用

#### 提示工程 vs RAG vs 微调
* 没问清除 - 提示词工程
* 缺乏背景知识 - RAG
* 能力不足 - 微调

### 什么是RAG
* RAG是Retrieval-Augmented Generation的缩写，它是一种基于检索的生成模型。
* RAG的基本思想是先从一个大的文本 corpus 中检索出与输入相关的信息，然后将这些信息作为上下文，与输入一起输入到一个生成模型中，生成符合上下文的输出。
* RAG的优势在于它可以利用外部的知识库来增强生成模型的能力，从而生成更符合上下文的输出。
* RAG在自然语言处理、计算机视觉等领域有广泛的应用，例如推荐系统、图像检索等。
* 提升专业领域回答质量
  * 专业领域的回答通常需要专业知识和背景，而RAG可以利用外部的知识库来提供专业知识，从而提升回答质量。
  * 例如，在医疗领域，RAG可以利用医学知识库来回答患者的问题，从而提供更准确的回答。

### RAG流程
#### 1. 数据预处理
* 知识库构建
* 文档分块
  * 将文档分成多个小块，每个小块包含一定数量的文本。
  * 每个小块都有一个唯一的ID，用于后续的检索。
* 向量化处理
  * 对每个文档块进行向量化处理，将其转换为向量表示。
  * 常用的向量化模型包括BERT、GPT等。

#### 2. 检索
* 接收用户输入作为查询。
* 利用向量数据库对查询进行向量化处理，将其转换为向量表示。
* 从向量数据库中检索出与查询向量最相似的文档块。
* 将检索到的文档块作为上下文。

#### 3. 生成
* 利用检索到的上下文和输入，调用生成模型生成符合上下文的输出。
* 常用的生成模型包括BERT、GPT等。

#### 4. 输出
* 将生成的输出返回给用户。

### 什么是NoteBookLLM
* NoteBookLLM是一种基于RAG技术的生成模型，它可以利用外部的知识库来增强生成模型的能力，从而生成更符合上下文的输出。
* NoteBookLLM在自然语言处理、计算机视觉等领域有广泛的应用，例如推荐系统、图像检索等。

### 常见的现成的RAG产品
* Cherry Stuido - 开源的
* imacopilot - 腾讯的
* notebookllm

### Embedding模型选择
#### 什么是huggingface
* huggingface是一个开源的自然语言处理库，它提供了许多预训练的模型，例如BERT、GPT等。
* 我们可以利用huggingface提供的预训练模型，来将文本转换为向量表示，从而用于RAG技术中。
#### 有名的Embedding模型
* BGE-M3
  * 特点：支持100+语言
* text-embedding-3-large(OpenAI) 
  * 特点：向量维度3072，长文本语义捕捉能力强，英文表现优秀，
  * 适用场景： 复杂指令驱动任务，智能问答系统
* E5-mistral-7B(Microsoft)

#### 用户的query会包括
1. instruction指令
2. 知识
* 帮我查找XXX

#### Embidding模型怎么部署
* 可以将Embedding模型部署在本地，也可以部署在云平台上，例如AWS、GCP等。
* 部署在本地时，需要安装相应的库，例如huggingface-transformers等。
* 部署在云平台上时，需要根据平台的要求，配置相应的环境和参数。

#### 向量数据库中，可以有两个阶段
1. 召回
2. 量排
  * 向量数据库中保存的chunk可能会有1000万个-> 召回快速筛选出1000个 -> 重排序top10

#### DeepSeek + Faiss搭建本地知识库检索
* DeepSeek是一个基于RAG技术的生成模型，它可以利用外部的知识库来增强生成模型的能力，从而生成更符合上下文的输出。
* Faiss是一个基于向量数据库的检索系统，它可以快速地检索出与查询向量最相似的文档块。
* 我们可以利用DeepSeek和Faiss搭建一个本地的知识库检索系统，用于回答用户的问题。

#### DeepSeek + Faiss代码中，使用了两个模型
1. 推理模型 qwen-turbo
2. Embedding模型 text-embedding-v1

##### chunk策略
* chunk_size = 1000, overlap = 200, 分割是按照标点符号进行分割(句号，换行)
* docs = knowledgeBase.similarity_search(query)

##### 为什么要分块？
* 因为RAG技术是基于文档块的检索，而不是基于整个文档的检索。
* 因此，我们需要将文档分成多个小块，每个小块包含一定数量的文本。
* 每个小块都有一个唯一的ID，用于后续的检索。

##### 企业做自己的知识库，会用这些大厂做的平台吗
* coze有企业版本 4980一个月
* dify开源
* LangChain实现了qa_chain，可以用faiss做向量数据库

### QA
* 如果知识库回答不了的，在调用推理模型吗？
* 如果llm中没有相关知识，rag是不是就没有效果了？
* chunk是什么样的格式存储的？
* 如果LLM可以处理无限上下文了，RAG还有意思吗？

### 总结
1. PDF文本提取与处理
2. 向量数据库构建


### LangChain中的问答链
1. stuff
2. map_reduce
  * 将每个document单独处理，可以并发进行调用
3. refine
  * 这种方式能部分保留上下文，以及token的使用能控制在一定范围
4. map_rerank
  * 会大量的调用LLM，每个document之间独立处理

#### RAG 
* 效率与成本
* 知识更新
* 可解释性
* 定制化
* 数据隐私

#### RAG的FAQ
* 如何构建完整的数据准备流程?
* 如何提升知识检索阶段的质量？
* RAG答案生成：
  * 改进提示词模板
  * 实施动态防护栏



#### 向量数据库和embedding模型是什么关系
* 向量数据库和embedding模型是两个不同的概念，但是它们之间有密切的联系。
* embedding模型是将文本转换为向量表示的模型，例如BERT、GPT等。
* 向量数据库是用于存储和管理向量数据的数据库，例如Faiss、Milvus等。
* 我们可以利用embedding模型将文本转换为向量表示，然后将这些向量存储在向量数据库中。
* 当用户输入一个查询时，我们可以利用embedding模型将查询转换为向量表示，然后利用向量数据库检索出与查询向量最相似的文档块。

#### LLM可以兼容RAG吗，RAG的优势怎么集成到LLM中？
* LLM可以兼容RAG，因为RAG是基于检索的生成模型，而LLM是基于生成的模型。
* 我们可以将RAG的检索阶段集成到LLM中，从而利用RAG的优势，例如利用外部的知识库来增强生成模型的能力。
















