"""
Ollama + Faiss + Rerank æœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢ç³»ç»Ÿ
============================================

ä½¿ç”¨æœ¬åœ° Ollama æ¨¡å‹ + Faiss + Rerank æ„å»ºçš„æœ¬åœ°çŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿã€‚
æ¼”ç¤º RAG é«˜æ•ˆå¬å›æ–¹æ³•ï¼šé‡æ’åºï¼ˆRerankingï¼‰

åŠŸèƒ½ï¼š
1. ä½¿ç”¨ Ollama Embedding ç”Ÿæˆå‘é‡
2. ä½¿ç”¨ Faiss è¿›è¡Œç²—æ’ï¼ˆå¿«é€Ÿå¬å› top-50ï¼‰
3. ä½¿ç”¨ Rerank è¿›è¡Œç²¾æ’ï¼ˆæé«˜å‡†ç¡®åº¦ï¼‰
4. æ£€ç´¢ç›¸å…³æ–‡æ¡£å¹¶ç”¨ Ollama ç”Ÿæˆç­”æ¡ˆ

ä½œè€…: Claude Code Assistant
æ—¥æœŸ: 2026-01-27
"""

import os
import json
import pickle
from typing import List, Dict, Tuple
from pathlib import Path
import re
import requests

import numpy as np
import faiss

# ============================================================================
# é…ç½®
# ============================================================================

OLLAMA_BASE_URL = "http://localhost:11434"
EMBEDDING_MODEL = "nomic-embed-text"  # Ollama çš„ embedding æ¨¡å‹
CHAT_MODEL = "deepseek-r1:1.5b"       # ä½ çš„æœ¬åœ°æ¨¡å‹

# ============================================================================
# 1. æ–‡æ¡£åŠ è½½å’Œé¢„å¤„ç†
# ============================================================================

class DocumentLoader:
    """æ–‡æ¡£åŠ è½½å™¨ - æ”¯æŒå¤šç§æ ¼å¼"""

    @staticmethod
    def load_txt(file_path: str) -> str:
        """åŠ è½½ TXT æ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()

    @staticmethod
    def load_pdf(file_path: str) -> str:
        """åŠ è½½ PDF æ–‡ä»¶"""
        try:
            import PyPDF2
            with open(file_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                text = ""
                for page in reader.pages:
                    text += page.extract_text()
                return text
        except ImportError:
            print("âš ï¸  è¯·å®‰è£… PyPDF2: pip install PyPDF2")
            return ""

    @staticmethod
    def load_docx(file_path: str) -> str:
        """åŠ è½½ DOCX æ–‡ä»¶"""
        try:
            import docx
            doc = docx.Document(file_path)
            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            return text
        except ImportError:
            print("âš ï¸  è¯·å®‰è£… python-docx: pip install python-docx")
            return ""

    @staticmethod
    def load_file(file_path: str) -> str:
        """æ ¹æ®æ–‡ä»¶æ‰©å±•åè‡ªåŠ¨åŠ è½½"""
        ext = Path(file_path).suffix.lower()

        loaders = {
            '.txt': DocumentLoader.load_txt,
            '.pdf': DocumentLoader.load_pdf,
            '.docx': DocumentLoader.load_docx,
        }

        loader = loaders.get(ext)
        if loader:
            return loader(file_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {ext}")

    @staticmethod
    def load_directory(directory: str) -> List[Tuple[str, str]]:
        """åŠ è½½ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡æ¡£"""
        documents = []
        path = Path(directory)
        supported_exts = ['.txt', '.pdf', '.docx']

        for file_path in path.rglob('*'):
            if file_path.suffix.lower() in supported_exts:
                try:
                    content = DocumentLoader.load_file(str(file_path))
                    if content.strip():
                        documents.append((file_path.name, content))
                        print(f"âœ… å·²åŠ è½½: {file_path.name}")
                except Exception as e:
                    print(f"âŒ åŠ è½½å¤±è´¥ {file_path.name}: {e}")

        return documents


class TextSplitter:
    """æ–‡æœ¬åˆ†å‰²å™¨ - å°†æ–‡æ¡£åˆ‡åˆ†æˆå°å—"""

    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

    def split_text(self, text: str, metadata: str = "") -> List[Dict]:
        """åˆ†å‰²æ–‡æœ¬"""
        chunks = []
        text = re.sub(r'\n+', '\n', text)
        text = text.strip()

        paragraphs = text.split('\n\n')
        current_chunk = ""
        chunk_id = 0

        for para in paragraphs:
            para = para.strip()
            if not para:
                continue

            if len(current_chunk) + len(para) + 2 <= self.chunk_size:
                current_chunk += para + "\n\n"
            else:
                if current_chunk.strip():
                    chunks.append({
                        "content": current_chunk.strip(),
                        "metadata": metadata,
                        "chunk_id": chunk_id
                    })
                    chunk_id += 1

                if self.chunk_overlap > 0 and current_chunk:
                    overlap_text = current_chunk[-self.chunk_overlap:]
                    current_chunk = overlap_text + para + "\n\n"
                else:
                    current_chunk = para + "\n\n"

        if current_chunk.strip():
            chunks.append({
                "content": current_chunk.strip(),
                "metadata": metadata,
                "chunk_id": chunk_id
            })

        return chunks

    def split_documents(self, documents: List[Tuple[str, str]]) -> List[Dict]:
        """åˆ†å‰²å¤šä¸ªæ–‡æ¡£"""
        all_chunks = []
        for filename, content in documents:
            chunks = self.split_text(content, metadata=filename)
            all_chunks.extend(chunks)
        return all_chunks


# ============================================================================
# 2. Ollama Embedding
# ============================================================================

class OllamaEmbedding:
    """Ollama Embedding ç”Ÿæˆå™¨"""

    def __init__(self, base_url: str = OLLAMA_BASE_URL, model: str = EMBEDDING_MODEL):
        self.base_url = base_url
        self.model = model

        # æ£€æŸ¥ Ollama æ˜¯å¦è¿è¡Œ
        try:
            response = requests.get(f"{base_url}/api/tags", timeout=5)
            if response.status_code != 200:
                raise Exception("Ollama æœªè¿è¡Œ")
            print(f"âœ… Ollama è¿æ¥æˆåŠŸ")
        except Exception as e:
            raise Exception(f"æ— æ³•è¿æ¥åˆ° Ollama ({base_url}): {e}")

    def get_embedding(self, text: str) -> List[float]:
        """è·å–æ–‡æœ¬çš„ Embedding"""
        try:
            response = requests.post(
                f"{self.base_url}/api/embeddings",
                json={
                    "model": self.model,
                    "prompt": text
                },
                timeout=60
            )

            if response.status_code == 200:
                result = response.json()
                return result.get("embedding", [])
            else:
                print(f"âš ï¸  Embedding ç”Ÿæˆå¤±è´¥: {response.status_code}")
                return []

        except Exception as e:
            print(f"âš ï¸  Embedding ç”Ÿæˆå‡ºé”™: {e}")
            return []

    def get_embeddings_batch(self, texts: List[str], batch_size: int = 10) -> List[List[float]]:
        """æ‰¹é‡è·å– Embedding"""
        embeddings = []

        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            for text in batch:
                emb = self.get_embedding(text)
                if emb:
                    embeddings.append(emb)
                    print(f"âœ… ç”Ÿæˆ embedding {len(embeddings)}/{len(texts)}")
                else:
                    print(f"âš ï¸  è·³è¿‡æ— æ³•ç”Ÿæˆ embedding çš„æ–‡æœ¬")
                    # ç”¨é›¶å‘é‡å¡«å……
                    embeddings.append([0.0] * 768)

        return embeddings


# ============================================================================
# 3. Faiss ç´¢å¼•ï¼ˆç²—æ’ï¼‰
# ============================================================================

class FaissIndex:
    """Faiss å‘é‡ç´¢å¼•ç®¡ç†å™¨ - ç”¨äºç²—æ’"""

    def __init__(self, dimension: int = 768):  # Ollama nomic-embed-text æ˜¯ 768 ç»´
        self.dimension = dimension
        self.index = None
        self.chunks = []
        self.embeddings = None  # ä¿å­˜ embeddings ç”¨äº rerank

    def build_index(self, chunks: List[Dict], embeddings: List[List[float]]):
        """æ„å»º Faiss ç´¢å¼•"""
        self.chunks = chunks
        self.embeddings = np.array(embeddings, dtype='float32')
        embeddings_array = self.embeddings

        if embeddings_array.shape[1] != self.dimension:
            print(f"âš ï¸  å‘é‡ç»´åº¦ä¸åŒ¹é…: æœŸæœ› {self.dimension}, å®é™… {embeddings_array.shape[1]}")
            self.dimension = embeddings_array.shape[1]

        self.index = faiss.IndexFlatL2(self.dimension)
        self.index.add(embeddings_array)

        print(f"âœ… Faiss ç´¢å¼•æ„å»ºå®Œæˆ: {len(chunks)} ä¸ªæ–‡æ¡£å—")

    def save(self, index_path: str = "faiss_index.bin", data_path: str = "chunks.pkl"):
        """ä¿å­˜ç´¢å¼•å’Œæ•°æ®"""
        if self.index is None:
            raise ValueError("ç´¢å¼•æœªæ„å»ºï¼Œæ— æ³•ä¿å­˜")

        faiss.write_index(self.index, index_path)
        with open(data_path, 'wb') as f:
            pickle.dump({
                'chunks': self.chunks,
                'embeddings': self.embeddings
            }, f)

        print(f"âœ… ç´¢å¼•å·²ä¿å­˜: {index_path}, {data_path}")

    def load(self, index_path: str = "faiss_index.bin", data_path: str = "chunks.pkl"):
        """åŠ è½½ç´¢å¼•å’Œæ•°æ®"""
        self.index = faiss.read_index(index_path)
        with open(data_path, 'rb') as f:
            data = pickle.load(f)
            self.chunks = data['chunks']
            self.embeddings = data['embeddings']

        self.dimension = self.index.d
        print(f"âœ… ç´¢å¼•å·²åŠ è½½: {len(self.chunks)} ä¸ªæ–‡æ¡£å—")

    def search(self, query_embedding: List[float], top_k: int = 50) -> List[Dict]:
        """
        ç²—æ’ï¼šå¿«é€Ÿå¬å› top-k ä¸ªå€™é€‰æ–‡æ¡£

        æ³¨æ„ï¼šè¿™é‡Œå¬å›æ›´å¤šæ–‡æ¡£ï¼ˆå¦‚50ä¸ªï¼‰ï¼Œä¸º rerank åšå‡†å¤‡
        """
        if self.index is None:
            raise ValueError("ç´¢å¼•æœªæ„å»ºï¼Œè¯·å…ˆæ„å»ºæˆ–åŠ è½½ç´¢å¼•")

        query_array = np.array([query_embedding], dtype='float32')
        distances, indices = self.index.search(query_array, top_k)

        results = []
        for i, (idx, dist) in enumerate(zip(indices[0], distances[0])):
            if idx < len(self.chunks):
                chunk = self.chunks[idx].copy()
                chunk['coarse_score'] = float(dist)  # ç²—æ’åˆ†æ•°ï¼ˆL2è·ç¦»ï¼‰
                chunk['embedding'] = self.embeddings[idx]  # ä¿å­˜ embedding ç”¨äº rerank
                results.append(chunk)

        return results


# ============================================================================
# 4. Rerank æ¨¡å‹ï¼ˆç²¾æ’ï¼‰â­ æ–°å¢
# ============================================================================

class Reranker:
    """
    é‡æ’åºå™¨ - å¯¹ç²—æ’ç»“æœè¿›è¡Œç²¾æ’

    æ–¹æ³•1: å‘é‡ç›¸ä¼¼åº¦å¢å¼ºï¼ˆå¿«é€Ÿï¼‰
    æ–¹æ³•2: å…³é”®è¯åŒ¹é…å¢å¼ºï¼ˆå¿«é€Ÿï¼‰
    æ–¹æ³•3: ä½¿ç”¨æ¨¡å‹æ‰“åˆ†ï¼ˆå‡†ç¡®ä½†æ…¢ï¼‰
    """

    def __init__(self, method: str = "vector"):
        """
        å‚æ•°:
            method: rerank æ–¹æ³•
                - "vector": åŸºäºå‘é‡ç›¸ä¼¼åº¦ï¼ˆæ¨èï¼Œæœ€å¿«ï¼‰
                - "keyword": åŸºäºå…³é”®è¯åŒ¹é…
                - "model": ä½¿ç”¨ LLM æ‰“åˆ†ï¼ˆå‡†ç¡®ä½†æ…¢ï¼‰
        """
        self.method = method

    def _vector_rerank(self, query_embedding: List[float], documents: List[Dict]) -> List[Dict]:
        """
        åŸºäºå‘é‡ç›¸ä¼¼åº¦çš„ rerank

        åŸç†ï¼š
        1. ä½¿ç”¨æŸ¥è¯¢å‘é‡å’Œæ–‡æ¡£å‘é‡è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        2. ç»“åˆç²—æ’åˆ†æ•°ï¼ˆL2è·ç¦»ï¼‰é‡æ–°æ’åº
        3. ä½™å¼¦ç›¸ä¼¼åº¦æ¯” L2 è·ç¦»æ›´é€‚åˆ rerank
        """
        query_vec = np.array(query_embedding).reshape(1, -1)

        for doc in documents:
            doc_vec = doc['embedding'].reshape(1, -1)

            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            cosine_sim = np.dot(query_vec, doc_vec.T) / (
                np.linalg.norm(query_vec) * np.linalg.norm(doc_vec)
            )

            # è½¬æ¢ L2 è·ç¦»ä¸ºç›¸ä¼¼åº¦ï¼ˆè¶Šå°è¶Šå¥½ â†’ è¶Šå¤§è¶Šå¥½ï¼‰
            l2_dist = doc['coarse_score']
            l2_sim = 1 / (1 + l2_dist)  # è½¬æ¢

            # ç»“åˆä¸¤ç§ç›¸ä¼¼åº¦
            doc['rerank_score'] = 0.7 * cosine_sim[0][0] + 0.3 * l2_sim
            doc['cosine_sim'] = cosine_sim[0][0]

        # æŒ‰ rerank åˆ†æ•°æ’åº
        reranked = sorted(documents, key=lambda x: x['rerank_score'], reverse=True)

        return reranked

    def _keyword_rerank(self, query: str, documents: List[Dict]) -> List[Dict]:
        """
        åŸºäºå…³é”®è¯åŒ¹é…çš„ rerank
        """
        query_keywords = set(query.lower().split())

        for doc in documents:
            content = doc['content'].lower()

            # è®¡ç®—å…³é”®è¯åŒ¹é…åˆ†æ•°
            keyword_matches = 0
            for keyword in query_keywords:
                if keyword in content:
                    keyword_matches += content.count(keyword)

            # ç»“åˆç²—æ’åˆ†æ•°å’Œå…³é”®è¯åŒ¹é…
            l2_dist = doc['coarse_score']
            l2_sim = 1 / (1 + l2_dist)
            doc['rerank_score'] = l2_sim * (1 + keyword_matches * 0.1)
            doc['keyword_matches'] = keyword_matches

        reranked = sorted(documents, key=lambda x: x['rerank_score'], reverse=True)
        return reranked

    def _model_rerank(self, query: str, documents: List[Dict]) -> List[Dict]:
        """
        ä½¿ç”¨ LLM è¿›è¡Œ rerank
        """
        print(f"ğŸ”„ ä½¿ç”¨æ¨¡å‹è¿›è¡Œ rerank ({len(documents)} ä¸ªæ–‡æ¡£)...")

        for i, doc in enumerate(documents):
            prompt = f"""è¯·è¯„åˆ†æŸ¥è¯¢å’Œæ–‡æ¡£çš„ç›¸å…³æ€§ï¼ˆ0-10åˆ†ï¼‰ï¼š

æŸ¥è¯¢ï¼š{query}

æ–‡æ¡£ï¼š{doc['content'][:200]}...

è¯·åªè¾“å‡ºä¸€ä¸ª0-10çš„æ•°å­—åˆ†æ•°ï¼š"""

            try:
                response = requests.post(
                    f"{OLLAMA_BASE_URL}/api/generate",
                    json={
                        "model": CHAT_MODEL,
                        "prompt": prompt,
                        "stream": False,
                        "options": {"num_predict": 5}
                    },
                    timeout=30
                )

                if response.status_code == 200:
                    result = response.json()
                    score_text = result.get("response", "5").strip()

                    # æå–æ•°å­—
                    import re
                    score_match = re.search(r'\d+(\.\d+)?', score_text)
                    if score_match:
                        score = float(score_match.group())
                    else:
                        score = 5.0

                    doc['rerank_score'] = score / 10.0
                else:
                    l2_sim = 1 / (1 + doc['coarse_score'])
                    doc['rerank_score'] = l2_sim

                print(f"  [{i+1}/{len(documents)}] æ‰“åˆ†å®Œæˆ: {doc['rerank_score']:.2f}")

            except Exception as e:
                print(f"  âš ï¸  æ–‡æ¡£ {i+1} æ‰“åˆ†å¤±è´¥: {e}")
                l2_sim = 1 / (1 + doc['coarse_score'])
                doc['rerank_score'] = l2_sim

        reranked = sorted(documents, key=lambda x: x['rerank_score'], reverse=True)
        return reranked

    def rerank(self, query: str, query_embedding: List[float], documents: List[Dict]) -> List[Dict]:
        """
        å¯¹æ–‡æ¡£è¿›è¡Œé‡æ’åº

        å‚æ•°:
            query: ç”¨æˆ·æŸ¥è¯¢
            query_embedding: æŸ¥è¯¢çš„å‘é‡
            documents: ç²—æ’ç»“æœåˆ—è¡¨

        è¿”å›:
            é‡æ’åºåçš„æ–‡æ¡£åˆ—è¡¨
        """
        if self.method == "vector":
            print("ğŸ”„ ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦è¿›è¡Œ rerank...")
            return self._vector_rerank(query_embedding, documents)
        elif self.method == "keyword":
            print("ğŸ”„ ä½¿ç”¨å…³é”®è¯åŒ¹é…è¿›è¡Œ rerank...")
            return self._keyword_rerank(query, documents)
        elif self.method == "model":
            return self._model_rerank(query, documents)
        else:
            raise ValueError(f"æœªçŸ¥çš„ rerank æ–¹æ³•: {self.method}")


# ============================================================================
# 5. Ollama é—®ç­”ç”Ÿæˆ
# ============================================================================

class OllamaChat:
    """Ollama é—®ç­”ç”Ÿæˆå™¨"""

    def __init__(self, base_url: str = OLLAMA_BASE_URL, model: str = CHAT_MODEL):
        self.base_url = base_url
        self.model = model

        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨
        try:
            response = requests.get(f"{base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get("models", [])
                model_names = [m.get("name", "") for m in models]
                print(f"âœ… Ollama å¯ç”¨æ¨¡å‹: {', '.join(model_names)}")

                if not any(model in m for m in model_names):
                    print(f"âš ï¸  è­¦å‘Š: æ¨¡å‹ '{model}' æœªæ‰¾åˆ°")
                    print(f"ğŸ’¡ è¿è¡Œ: ollama pull {model}")
        except Exception as e:
            print(f"âš ï¸  æ— æ³•è¿æ¥åˆ° Ollama: {e}")

    def generate_answer(self, question: str, context: str) -> str:
        """åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ"""
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹å‚è€ƒæ–‡æ¡£å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

å‚è€ƒæ–‡æ¡£ï¼š
{context}

é—®é¢˜ï¼š{question}

è¦æ±‚ï¼š
1. ç­”æ¡ˆå¿…é¡»åŸºäºå‚è€ƒæ–‡æ¡£ä¸­çš„ä¿¡æ¯
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜
3. ä¿æŒç­”æ¡ˆç®€æ´å‡†ç¡®

ç­”æ¡ˆï¼š"""

        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.3,
                        "num_predict": 1000
                    }
                },
                timeout=120
            )

            if response.status_code == 200:
                result = response.json()
                return result.get("response", "âš ï¸  æ— æ³•ç”Ÿæˆç­”æ¡ˆ")
            else:
                return f"âš ï¸  ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {response.status_code}"

        except Exception as e:
            return f"âš ï¸  ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {e}"


# ============================================================================
# 6. å®Œæ•´çš„ RAG ç³»ç»Ÿï¼ˆå¸¦ Rerankï¼‰
# ============================================================================

class RAGSystemWithRerank:
    """å¸¦ Rerank çš„ RAG ç³»ç»Ÿ"""

    def __init__(self, rerank_method: str = "vector"):
        self.embedder = OllamaEmbedding()
        self.chat = OllamaChat()
        self.index = FaissIndex()
        self.reranker = Reranker(method=rerank_method)

    def build_knowledge_base(self, documents_dir: str):
        """æ„å»ºçŸ¥è¯†åº“"""
        print("\n" + "=" * 80)
        print("ğŸ“š ç¬¬ä¸€æ­¥ï¼šåŠ è½½æ–‡æ¡£")
        print("=" * 80)

        loader = DocumentLoader()
        documents = loader.load_directory(documents_dir)

        if not documents:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£")
            return

        print(f"\nâœ… å…±åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")

        print("\n" + "=" * 80)
        print("âœ‚ï¸  ç¬¬äºŒæ­¥ï¼šåˆ†å‰²æ–‡æœ¬")
        print("=" * 80)

        splitter = TextSplitter(chunk_size=500, chunk_overlap=50)
        chunks = splitter.split_documents(documents)

        print(f"âœ… å…±åˆ†å‰²æˆ {len(chunks)} ä¸ªæ–‡æœ¬å—")

        print("\n" + "=" * 80)
        print("ğŸ”„ ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆ Embeddingï¼ˆä½¿ç”¨ Ollamaï¼‰")
        print("=" * 80)

        texts = [chunk['content'] for chunk in chunks]
        embeddings = self.embedder.get_embeddings_batch(texts, batch_size=10)

        print("\n" + "=" * 80)
        print("ğŸ”¨ ç¬¬å››æ­¥ï¼šæ„å»º Faiss ç´¢å¼•")
        print("=" * 80)

        self.index.build_index(chunks, embeddings)

        print("\n" + "=" * 80)
        print("ğŸ’¾ ç¬¬äº”æ­¥ï¼šä¿å­˜ç´¢å¼•")
        print("=" * 80)

        self.index.save()
        print("\nâœ¨ çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼")

    def load_knowledge_base(self):
        """åŠ è½½å·²æ„å»ºçš„çŸ¥è¯†åº“"""
        self.index.load()
        print("âœ… çŸ¥è¯†åº“å·²åŠ è½½")

    def query(self, question: str, coarse_top_k: int = 50, final_top_k: int = 3) -> Dict:
        """
        æŸ¥è¯¢çŸ¥è¯†åº“ï¼ˆå¸¦ Rerankï¼‰

        å‚æ•°:
            question: ç”¨æˆ·é—®é¢˜
            coarse_top_k: ç²—æ’å¬å›æ•°é‡ï¼ˆé»˜è®¤50ï¼‰
            final_top_k: æœ€ç»ˆè¿”å›æ•°é‡ï¼ˆé»˜è®¤3ï¼‰
        """
        print(f"\nğŸ” æŸ¥è¯¢é—®é¢˜: {question}")

        # 1ï¸âƒ£ ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = self.embedder.get_embedding(question)
        if not query_embedding:
            return {"error": "æ— æ³•ç”ŸæˆæŸ¥è¯¢å‘é‡"}

        # 2ï¸âƒ£ ç²—æ’ï¼šå‘é‡æ£€ç´¢å¬å›
        print(f"\nğŸ“Š ç¬¬ä¸€æ­¥ï¼šç²—æ’ï¼ˆå‘é‡æ£€ç´¢ï¼Œå¬å› top-{coarse_top_k}ï¼‰")
        coarse_results = self.index.search(query_embedding, top_k=coarse_top_k)
        print(f"âœ… ç²—æ’å®Œæˆï¼Œå¬å› {len(coarse_results)} ä¸ªå€™é€‰æ–‡æ¡£")

        # æ˜¾ç¤ºç²—æ’ top-3
        print("\nç²—æ’ Top-3:")
        for i, result in enumerate(coarse_results[:3]):
            print(f"  [{i+1}] {result['metadata']} (L2è·ç¦»: {result['coarse_score']:.4f})")

        # 3ï¸âƒ£ ç²¾æ’ï¼šRerank
        print(f"\nğŸ¯ ç¬¬äºŒæ­¥ï¼šç²¾æ’ï¼ˆRerankï¼‰")
        reranked_results = self.reranker.rerank(question, query_embedding, coarse_results)
        print(f"âœ… ç²¾æ’å®Œæˆ")

        # æ˜¾ç¤ºç²¾æ’å top-3
        print("\nç²¾æ’ Top-3:")
        for i, result in enumerate(reranked_results[:3]):
            print(f"  [{i+1}] {result['metadata']} (rerankåˆ†æ•°: {result.get('rerank_score', 0):.4f})")

        # 4ï¸âƒ£ å–æœ€ç»ˆ top-k
        final_results = reranked_results[:final_top_k]

        # 5ï¸âƒ£ ç»„è£…ä¸Šä¸‹æ–‡
        print(f"\nğŸ“š æœ€ç»ˆé€‰ä¸­çš„ {len(final_results)} ä¸ªæ–‡æ¡£:")
        for i, result in enumerate(final_results):
            print(f"\n  [{i+1}] {result['metadata']}")
            print(f"      ç²—æ’åˆ†æ•°: {result['coarse_score']:.4f}")
            print(f"      ç²¾æ’åˆ†æ•°: {result.get('rerank_score', 0):.4f}")
            if 'cosine_sim' in result:
                print(f"      ä½™å¼¦ç›¸ä¼¼åº¦: {result['cosine_sim']:.4f}")
            if 'keyword_matches' in result:
                print(f"      å…³é”®è¯åŒ¹é…: {result['keyword_matches']}")
            print(f"      å†…å®¹: {result['content'][:100]}...")

        context = "\n\n".join([
            f"ã€æ¥æº: {r['metadata']}ã€‘\n{r['content']}"
            for r in final_results
        ])

        # 6ï¸âƒ£ ç”Ÿæˆç­”æ¡ˆ
        print("\nğŸ’­ æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...")
        answer = self.chat.generate_answer(question, context)

        return {
            "answer": answer,
            "sources": final_results,
            "coarse_results": coarse_results,
            "query": question
        }


# ============================================================================
# 7. ä¸»ç¨‹åº
# ============================================================================

def main():
    """ä¸»ç¨‹åºæ¼”ç¤º"""

    print("=" * 80)
    print("ğŸ¤– Ollama + Faiss + Rerank æœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢ç³»ç»Ÿ")
    print("   æ¼”ç¤º RAG é«˜æ•ˆå¬å›æ–¹æ³•ï¼šé‡æ’åºï¼ˆRerankingï¼‰")
    print("=" * 80)

    # æ£€æŸ¥ Ollama æ˜¯å¦è¿è¡Œ
    try:
        response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5)
        if response.status_code != 200:
            print("\nâŒ é”™è¯¯ï¼šOllama æœªè¿è¡Œ")
            print("\nè¯·å…ˆå¯åŠ¨ Ollamaï¼š")
            print("  - macOS: æ‰“å¼€ Ollama åº”ç”¨")
            print("  - æˆ–è¿è¡Œ: ollama serve")
            return
    except Exception as e:
        print(f"\nâŒ é”™è¯¯ï¼šæ— æ³•è¿æ¥åˆ° Ollama")
        print(f"ğŸ’¡ è¯·ç¡®ä¿ Ollama æ­£åœ¨è¿è¡Œ: {OLLAMA_BASE_URL}")
        return

    # é€‰æ‹© Rerank æ–¹æ³•
    print("\nè¯·é€‰æ‹© Rerank æ–¹æ³•:")
    print("1. å‘é‡ç›¸ä¼¼åº¦ï¼ˆæœ€å¿«ï¼Œæ¨èï¼‰")
    print("2. å…³é”®è¯åŒ¹é…")
    print("3. æ¨¡å‹æ‰“åˆ†ï¼ˆå‡†ç¡®ä½†æ…¢ï¼‰")
    print("\nè¾“å…¥ 1-3ï¼ˆé»˜è®¤ 1ï¼‰: ", end="")

    try:
        choice = input().strip()
        if choice == "2":
            rerank_method = "keyword"
            print("\nâœ… ä½¿ç”¨å…³é”®è¯åŒ¹é…æ–¹æ³•")
        elif choice == "3":
            rerank_method = "model"
            print("\nâœ… ä½¿ç”¨æ¨¡å‹æ‰“åˆ†æ–¹æ³•")
        else:
            rerank_method = "vector"
            print("\nâœ… ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦æ–¹æ³•ï¼ˆé»˜è®¤ï¼‰")
    except:
        rerank_method = "vector"
        print("\nâœ… ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦æ–¹æ³•ï¼ˆé»˜è®¤ï¼‰")

    # åˆ›å»º RAG ç³»ç»Ÿ
    try:
        rag = RAGSystemWithRerank(rerank_method=rerank_method)
    except Exception as e:
        print(f"\nâŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        print("\nğŸ’¡ è¯·ç¡®ä¿å·²ä¸‹è½½ embedding æ¨¡å‹: ollama pull nomic-embed-text")
        return

    # åˆ›å»ºç¤ºä¾‹æ–‡æ¡£ç›®å½•
    docs_dir = "knowledge_base"
    Path(docs_dir).mkdir(exist_ok=True)

    # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ–‡æ¡£
    if not list(Path(docs_dir).rglob('*')):
        print(f"\nğŸ“ åœ¨ {docs_dir}/ ç›®å½•ä¸­æ·»åŠ ä½ çš„æ–‡æ¡£ï¼ˆtxt, pdf, docxï¼‰")
        print("ç„¶åé‡æ–°è¿è¡Œç¨‹åº")
        return

    # æ„å»ºæˆ–åŠ è½½çŸ¥è¯†åº“
    if Path("faiss_index.bin").exists():
        print("\næ£€æµ‹åˆ°å·²æœ‰ç´¢å¼•ï¼Œæ˜¯å¦é‡æ–°æ„å»ºï¼Ÿ(y/n): ", end="")
        try:
            choice = input().strip().lower()
            if choice == 'y':
                rag.build_knowledge_base(docs_dir)
            else:
                rag.load_knowledge_base()
        except:
            rag.load_knowledge_base()
    else:
        rag.build_knowledge_base(docs_dir)

    # äº¤äº’å¼é—®ç­”
    print("\n" + "=" * 80)
    print("ğŸ’¬ å¼€å§‹é—®ç­”ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰")
    print("=" * 80)

    while True:
        print("\n" + "â”€" * 80)
        try:
            question = input("â“ ä½ çš„é—®é¢˜: ").strip()

            if not question:
                continue

            if question.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ å†è§ï¼")
                break

            # æŸ¥è¯¢ï¼ˆå¸¦ rerankï¼‰
            result = rag.query(question, coarse_top_k=50, final_top_k=3)

            # æ˜¾ç¤ºç­”æ¡ˆ
            if "error" in result:
                print(f"\nâŒ {result['error']}")
                continue

            print("\n" + "â”€" * 80)
            print("ğŸ“– ç­”æ¡ˆ:")
            print("â”€" * 80)
            print(result['answer'])
            print("â”€" * 80)

            # æ˜¾ç¤ºå¯¹æ¯”
            print("\nğŸ“Š ç²—æ’ vs ç²¾æ’å¯¹æ¯”ï¼ˆTop-3ï¼‰:")
            coarse_top3 = result['coarse_results'][:3]
            final_top3 = result['sources']

            print("\nç²—æ’ Top-3 (L2è·ç¦»ï¼Œè¶Šå°è¶Šå¥½):")
            for i, doc in enumerate(coarse_top3):
                print(f"  {i+1}. {doc['metadata']} ({doc['coarse_score']:.4f})")

            print("\nç²¾æ’ Top-3 (rerankåˆ†æ•°ï¼Œè¶Šå¤§è¶Šå¥½):")
            for i, doc in enumerate(final_top3):
                print(f"  {i+1}. {doc['metadata']} ({doc.get('rerank_score', 0):.4f})")

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ å‡ºé”™äº†: {e}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    main()
