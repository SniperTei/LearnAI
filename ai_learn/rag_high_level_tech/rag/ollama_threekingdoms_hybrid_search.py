"""
ä¸‰å›½çŸ¥è¯†åº“ + æ··åˆæ£€ç´¢ç³»ç»Ÿ
==========================

ä½¿ç”¨æœ¬åœ° Ollama æ¨¡å‹ + æ··åˆæ£€ç´¢æ„å»ºçš„ä¸‰å›½çŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿã€‚
æ¼”ç¤º RAG é«˜æ•ˆå¬å›æ–¹æ³•ï¼šæ··åˆæ£€ç´¢ï¼ˆHybrid Searchï¼‰

æ··åˆæ£€ç´¢ = å‘é‡æ£€ç´¢ï¼ˆFAISSï¼‰+ å…³é”®è¯æ£€ç´¢ï¼ˆBM25ï¼‰+ ç»“æœèåˆï¼ˆRRFï¼‰

åŠŸèƒ½ï¼š
1. FAISS å‘é‡æ£€ç´¢ - è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…
2. BM25 å…³é”®è¯æ£€ç´¢ - ç²¾ç¡®å…³é”®è¯åŒ¹é…
3. RRF ç»“æœèåˆ - ç»“åˆä¸¤ç§æ£€ç´¢ç»“æœ
4. å¯¹æ¯”ä¸‰ç§æ£€ç´¢æ–¹å¼çš„æ•ˆæœ

ç‰¹ç‚¹ï¼š
- å‘é‡æ£€ç´¢æ“…é•¿è¯­ä¹‰ç†è§£ï¼ˆ"ä¸‰å›½æ¼”ä¹‰" â†” "ä¸‰å›½"ï¼‰
- BM25æ“…é•¿å…³é”®è¯åŒ¹é…ï¼ˆ"è¯¸è‘›äº®" â†” "å­”æ˜"ï¼‰
- ä¸¤è€…äº’è¡¥ï¼Œæ£€ç´¢æ•ˆæœæ˜¾è‘—æå‡

ä½œè€…: Claude Code Assistant
æ—¥æœŸ: 2026-01-31
"""

import os
import json
import pickle
import math
from typing import List, Dict, Tuple
from pathlib import Path
import re
import requests
from collections import Counter

import numpy as np
import faiss

# ============================================================================
# é…ç½®
# ============================================================================

OLLAMA_BASE_URL = "http://localhost:11434"
EMBEDDING_MODEL = "nomic-embed-text"
CHAT_MODEL = "deepseek-r1:7b"

DOCS_DIR = "knowledge_threekingdoms"
INDEX_PREFIX = "threekingdoms_hybrid"
CHUNK_SIZE = 800
CHUNK_OVERLAP = 100
VECTOR_TOP_K = 50
BM25_TOP_K = 50
FINAL_TOP_K = 5

# ============================================================================
# æ–‡æ¡£åŠ è½½å’Œé¢„å¤„ç†
# ============================================================================

class DocumentLoader:
    """æ–‡æ¡£åŠ è½½å™¨"""

    @staticmethod
    def load_txt(file_path: str) -> str:
        """åŠ è½½ TXT æ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()

    @staticmethod
    def load_directory(directory: str) -> List[Tuple[str, str]]:
        """åŠ è½½ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡æ¡£"""
        documents = []
        path = Path(directory)

        for file_path in path.rglob('*.txt'):
            try:
                content = DocumentLoader.load_txt(str(file_path))
                if content.strip():
                    documents.append((file_path.name, content))
                    print(f"âœ… å·²åŠ è½½: {file_path.name} ({len(content):,} å­—ç¬¦)")
            except Exception as e:
                print(f"âŒ åŠ è½½å¤±è´¥ {file_path.name}: {e}")

        return documents


class TextSplitter:
    """æ™ºèƒ½æ–‡æœ¬åˆ†å‰²å™¨ - æŒ‰æ®µè½/å¥å­åˆ†å—"""

    def __init__(self, chunk_size: int = CHUNK_SIZE, chunk_overlap: int = CHUNK_OVERLAP):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

    def split_text(self, text: str, metadata: str = "") -> List[Dict]:
        """
        æ™ºèƒ½åˆ†å‰²æ–‡æœ¬ï¼ˆæŒ‰æ®µè½ä¼˜å…ˆï¼‰

        ç­–ç•¥ï¼š
        1. ä¼˜å…ˆæŒ‰æ®µè½ï¼ˆ\n\nï¼‰åˆ†å—
        2. æ®µè½å¤ªé•¿æ—¶ï¼ŒæŒ‰å¥å­ï¼ˆã€‚ï¼‰åˆ‡åˆ†
        3. ä¿æŒè¯­ä¹‰å®Œæ•´æ€§
        """
        chunks = []
        chunk_id = 0

        # ç¬¬ä¸€æ­¥ï¼šæŒ‰æ®µè½åˆ†å‰²ï¼ˆä¿ç•™å•æ¢è¡Œä½œä¸ºæ®µè½å†…çš„æ¢è¡Œï¼‰
        paragraphs = text.split('\n\n')

        current_chunk = ""
        current_length = 0

        for para in paragraphs:
            para = para.strip()
            if not para:
                continue

            para = para.replace('\n', ' ')  # æ®µè½å†…çš„æ¢è¡Œæ›¿æ¢ä¸ºç©ºæ ¼
            para_length = len(para)

            # å¦‚æœå•ä¸ªæ®µè½å°±è¶…è¿‡äº†chunk_sizeï¼Œéœ€è¦åˆ‡åˆ†
            if para_length > self.chunk_size:
                # å…ˆä¿å­˜å½“å‰chunk
                if current_chunk:
                    chunks.append({
                        "content": current_chunk.strip(),
                        "metadata": metadata,
                        "chunk_id": chunk_id
                    })
                    chunk_id += 1
                    current_chunk = ""
                    current_length = 0

                # åˆ‡åˆ†é•¿æ®µè½ï¼ˆæŒ‰å¥å­ï¼‰
                sentences = self._split_long_paragraph(para)
                for sent in sentences:
                    if current_length + len(sent) + 2 <= self.chunk_size:
                        current_chunk += sent + "ã€‚"
                        current_length += len(sent) + 1
                    else:
                        if current_chunk:
                            chunks.append({
                                "content": current_chunk.strip(),
                                "metadata": metadata,
                                "chunk_id": chunk_id
                            })
                            chunk_id += 1
                        # æ·»åŠ é‡å 
                        if self.chunk_overlap > 0 and current_chunk:
                            overlap_text = current_chunk[-self.chunk_overlap:]
                            current_chunk = overlap_text + sent + "ã€‚"
                            current_length = len(current_chunk)
                        else:
                            current_chunk = sent + "ã€‚"
                            current_length = len(sent) + 1

            # å¦‚æœæ®µè½å¯ä»¥æ”¾å…¥å½“å‰chunk
            elif current_length + para_length + 2 <= self.chunk_size:
                current_chunk += "\n\n" + para
                current_length += para_length + 2

            # éœ€è¦æ–°çš„chunk
            else:
                # ä¿å­˜å½“å‰chunk
                if current_chunk:
                    chunks.append({
                        "content": current_chunk.strip(),
                        "metadata": metadata,
                        "chunk_id": chunk_id
                    })
                    chunk_id += 1

                # æ·»åŠ é‡å 
                if self.chunk_overlap > 0:
                    overlap_text = current_chunk[-self.chunk_overlap:]
                    current_chunk = overlap_text + "\n\n" + para
                    current_length = len(overlap_text) + para_length + 2
                else:
                    current_chunk = para
                    current_length = para_length

        # æœ€åä¸€ä¸ªchunk
        if current_chunk.strip():
            chunks.append({
                "content": current_chunk.strip(),
                "metadata": metadata,
                "chunk_id": chunk_id
            })

        return chunks

    def _split_long_paragraph(self, text: str) -> List[str]:
        """
        åˆ‡åˆ†é•¿æ®µè½ï¼ˆæŒ‰å¥å­ï¼‰

        ä¿æŒå¥å­å®Œæ•´æ€§
        """
        sentences = []
        current_sent = ""

        i = 0
        while i < len(text):
            char = text[i]

            # é‡åˆ°å¥å·ã€é—®å·ã€æ„Ÿå¹å·ï¼Œåˆ‡åˆ†
            if char in ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›']:
                current_sent += char
                if current_sent.strip():
                    sentences.append(current_sent.strip())
                current_sent = ""

            # é‡åˆ°æ¢è¡Œç¬¦ï¼Œä¹Ÿåˆ‡åˆ†
            elif char == '\n':
                if current_sent.strip():
                    sentences.append(current_sent.strip())
                current_sent = ""

            else:
                current_sent += char

            i += 1

        # æœ€åä¸€å¥
        if current_sent.strip():
            sentences.append(current_sent.strip())

        return sentences

    def split_documents(self, documents: List[Tuple[str, str]]) -> List[Dict]:
        """åˆ†å‰²å¤šä¸ªæ–‡æ¡£"""
        all_chunks = []
        for filename, content in documents:
            chunks = self.split_text(content, metadata=filename)
            all_chunks.extend(chunks)
        return all_chunks


# ============================================================================
# ä¸­æ–‡åˆ†è¯ï¼ˆä½¿ç”¨jieba + ä¸‰å›½ä¸“æœ‰è¯æ±‡ï¼‰
# ============================================================================

import jieba

class ChineseTokenizer:
    """ä½¿ç”¨jiebaçš„ä¸­æ–‡åˆ†è¯å™¨ï¼ˆåŠ è½½ä¸‰å›½è¯å…¸ï¼‰"""

    @staticmethod
    def load_threekingdoms_dict():
        """åŠ è½½ä¸‰å›½ä¸“æœ‰è¯æ±‡"""
        # ä¸‰å›½äººåã€åœ°åã€æ­¦å™¨åç­‰
        threekingdoms_words = [
            # äººå
            'è¯¸è‘›äº®', 'å­”æ˜', 'å§é¾™', 'åˆ˜å¤‡', 'ç„å¾·', 'å…³ç¾½', 'äº‘é•¿', 'å¼ é£', 'ç¿¼å¾·',
            'æ›¹æ“', 'å­Ÿå¾·', 'å­™æƒ', 'ä»²è°‹', 'å‘¨ç‘œ', 'å…¬ç‘¾', 'å•å¸ƒ', 'å¥‰å…ˆ', 'èµµäº‘',
            'å­é¾™', 'é»„å¿ ', 'æ±‰å‡', 'é©¬è¶…', 'å­Ÿèµ·', 'é­å»¶', 'æ–‡é•¿', 'å§œç»´', 'ä¼¯çº¦',
            'å¸é©¬æ‡¿', 'ä»²è¾¾', 'é™†é€Š', 'ä¼¯è¨€', 'å­™ç­–', 'ä¼¯ç¬¦', 'é»„ç›–', 'å…¬è¦†',
            'è‘£å“', 'ä»²é¢–', 'è¢ç»', 'æœ¬åˆ', 'è¢æœ¯', 'å…¬è·¯', 'åˆ˜è¡¨', 'æ™¯å‡',
            # åœ°å
            'èµ¤å£', 'è†å·', 'ç›Šå·', 'æ±Ÿä¸œ', 'ä¸­åŸ', 'æ´›é˜³', 'é•¿å®‰', 'æˆéƒ½',
            'å»ºä¸š', 'è®¸æ˜Œ', 'é‚ºåŸ', 'åˆè‚¥', 'æ¿¡é¡»', 'å¤·é™µ', 'äº”ä¸ˆåŸ',
            # æ­¦å™¨
            'é’é¾™åƒæœˆåˆ€', 'ä¸ˆå…«è›‡çŸ›', 'æ–¹å¤©ç”»æˆŸ', 'åŒè‚¡å‰‘', 'é›Œé›„åŒå‰‘',
            'ç¾½æ‰‡', 'é¹¤æ°…', 'èµ¤å…”é©¬', 'çš„å¢', 'ç»å½±',
            # æˆ˜å½¹
            'å®˜æ¸¡ä¹‹æˆ˜', 'èµ¤å£ä¹‹æˆ˜', 'å¤·é™µä¹‹æˆ˜', 'äº”ä¸ˆåŸ', 'æ¡ƒå›­ç»“ä¹‰',
            'ä¸‰é¡¾èŒ…åº', 'è‰èˆ¹å€Ÿç®­', 'ç©ºåŸè®¡', 'ç«çƒ§è¿è¥',
            # èŒä½
            'ä¸ç›¸', 'å¤ªå°‰', 'å¤§å°†å†›', 'éƒ½ç£', 'å¤ªå®ˆ', 'åˆºå²',
            # å…¶ä»–
            'ä¸‰å›½æ¼”ä¹‰', 'ä¸‰å›½å¿—', 'é­èœ€å´'
        ]

        for word in threekingdoms_words:
            jieba.add_word(word, freq=10000)  # é«˜é¢‘è¯

    @staticmethod
    def tokenize(text: str) -> List[str]:
        """
        ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯

        ä¼˜åŒ–ï¼š
        1. ä½¿ç”¨jiebaç²¾ç¡®æ¨¡å¼
        2. åŠ è½½ä¸‰å›½ä¸“æœ‰è¯æ±‡
        3. è¿‡æ»¤åœç”¨è¯ï¼ˆæ ‡ç‚¹ç¬¦å·ï¼‰
        4. ä¿ç•™æœ‰æ„ä¹‰çš„è¯
        """
        # é¦–æ¬¡ä½¿ç”¨æ—¶åŠ è½½è¯å…¸
        if not hasattr(ChineseTokenizer, '_dict_loaded'):
            ChineseTokenizer.load_threekingdoms_dict()
            ChineseTokenizer._dict_loaded = True

        # ä½¿ç”¨jiebaåˆ†è¯
        words = jieba.cut(text, cut_all=False)

        # è¿‡æ»¤åœç”¨è¯
        tokens = []
        stopwords = {' ', '\n', '\t', '\r', 'ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›', 'ï¼š', 'ã€',
                    'ã€Œ', 'ã€', 'ã€', 'ã€', 'ï¼ˆ', 'ï¼‰', 'ã€', 'ã€‘', 'ã€Š', 'ã€‹',
                    ',', '.', '!', '?', ';', ':', '"', '"', "'", "'",
                    'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'å’Œ', 'æœ‰', 'æˆ‘', 'ä½ ', 'ä»–', 'å¥¹',
                    'å®ƒ', 'ä»¬', 'è¿™', 'é‚£', 'å°±', 'ä¹Ÿ', 'éƒ½', 'è€Œ', 'åŠ', 'ä¸'}

        for word in words:
            word = word.strip()
            # è¿‡æ»¤å•å­—åœç”¨è¯å’Œæ ‡ç‚¹
            if word and word not in stopwords and len(word) > 0:
                # ä¿ç•™ï¼šä¸­æ–‡å­—ç¬¦ã€æ•°å­—ã€è‹±æ–‡
                if any('\u4e00' <= c <= '\u9fff' or c.isalnum() for c in word):
                    tokens.append(word)

        return tokens


# ============================================================================
# Ollama Embedding
# ============================================================================

class OllamaEmbedding:
    """Ollama Embedding ç”Ÿæˆå™¨"""

    def __init__(self):
        self.base_url = OLLAMA_BASE_URL
        self.model = EMBEDDING_MODEL

        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code != 200:
                raise Exception("Ollama æœªè¿è¡Œ")
            print(f"âœ… Ollama è¿æ¥æˆåŠŸ")
        except Exception as e:
            raise Exception(f"æ— æ³•è¿æ¥åˆ° Ollama: {e}")

    def get_embedding(self, text: str) -> List[float]:
        """è·å–æ–‡æœ¬çš„ Embedding"""
        try:
            response = requests.post(
                f"{self.base_url}/api/embeddings",
                json={"model": self.model, "prompt": text},
                timeout=60
            )

            if response.status_code == 200:
                return response.json().get("embedding", [])
            return []
        except:
            return []

    def get_embeddings_batch(self, texts: List[str], batch_size: int = 10) -> List[List[float]]:
        """æ‰¹é‡è·å– Embedding"""
        embeddings = []

        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            for text in batch:
                emb = self.get_embedding(text)
                if emb:
                    embeddings.append(emb)
                    print(f"âœ… [{len(embeddings)}/{len(texts)}] ç”Ÿæˆ embedding")
                else:
                    embeddings.append([0.0] * 768)

        return embeddings


# ============================================================================
# BM25 ç´¢å¼•
# ============================================================================

class BM25Index:
    """
    BM25 ç´¢å¼• - å…³é”®è¯æ£€ç´¢

    BM25 æ˜¯ä¸€ç§æ”¹è¿›çš„ TF-IDF ç®—æ³•ï¼Œè€ƒè™‘äº†ï¼š
    1. è¯é¢‘é¥±å’Œåº¦ï¼ˆè¯é¢‘è¶Šé«˜ï¼Œæƒé‡å¢é•¿è¶Šæ…¢ï¼‰
    2. æ–‡æ¡£é•¿åº¦å½’ä¸€åŒ–
    """

    def __init__(self, k1: float = 1.2, b: float = 0.75):
        """
        å‚æ•°:
            k1: è¯é¢‘é¥±å’Œåº¦å‚æ•°ï¼ˆ1.2æ›´ä¿å®ˆï¼Œé€‚åˆçŸ­æ–‡æ¡£ï¼‰
            b: é•¿åº¦å½’ä¸€åŒ–å‚æ•°ï¼ˆ0.75æ˜¯æ ‡å‡†å€¼ï¼‰
        """
        self.k1 = k1
        self.b = b
        self.chunks = []
        self.corpus = []  # åˆ†è¯åçš„æ–‡æ¡£
        self.doc_freqs = {}  # æ–‡æ¡£é¢‘ç‡
        self.idf = {}  # é€†æ–‡æ¡£é¢‘ç‡
        self.doc_lens = []  # æ–‡æ¡£é•¿åº¦
        self.avgdl = 0  # å¹³å‡æ–‡æ¡£é•¿åº¦

    def build_index(self, chunks: List[Dict]):
        """æ„å»º BM25 ç´¢å¼•"""
        self.chunks = chunks

        # åˆ†è¯
        print("ğŸ“ æ­£åœ¨åˆ†è¯...")
        for chunk in chunks:
            tokens = ChineseTokenizer.tokenize(chunk['content'])
            self.corpus.append(tokens)
            self.doc_lens.append(len(tokens))

        # è®¡ç®—å¹³å‡æ–‡æ¡£é•¿åº¦
        self.avgdl = sum(self.doc_lens) / len(self.doc_lens) if self.doc_lens else 0

        # è®¡ç®—æ–‡æ¡£é¢‘ç‡
        print("ğŸ“Š æ­£åœ¨è®¡ç®—æ–‡æ¡£é¢‘ç‡...")
        for tokens in self.corpus:
            unique_tokens = set(tokens)
            for token in unique_tokens:
                self.doc_freqs[token] = self.doc_freqs.get(token, 0) + 1

        # è®¡ç®— IDF
        print("ğŸ“ˆ æ­£åœ¨è®¡ç®— IDF...")
        N = len(self.corpus)
        for token, freq in self.doc_freqs.items():
            self.idf[token] = math.log((N - freq + 0.5) / (freq + 0.5) + 1)

        print(f"âœ… BM25 ç´¢å¼•æ„å»ºå®Œæˆ: {len(chunks)} ä¸ªæ–‡æ¡£å—")
        print(f"   è¯æ±‡é‡: {len(self.doc_freqs):,}")
        print(f"   å¹³å‡æ–‡æ¡£é•¿åº¦: {self.avgdl:.1f}")

    def save(self):
        """ä¿å­˜ç´¢å¼•"""
        with open(f"{INDEX_PREFIX}_bm25.pkl", 'wb') as f:
            pickle.dump({
                'chunks': self.chunks,
                'corpus': self.corpus,
                'doc_freqs': self.doc_freqs,
                'idf': self.idf,
                'doc_lens': self.doc_lens,
                'avgdl': self.avgdl,
                'k1': self.k1,
                'b': self.b
            }, f)
        print(f"âœ… BM25 ç´¢å¼•å·²ä¿å­˜: {INDEX_PREFIX}_bm25.pkl")

    def load(self):
        """åŠ è½½ç´¢å¼•"""
        with open(f"{INDEX_PREFIX}_bm25.pkl", 'rb') as f:
            data = pickle.load(f)
            self.chunks = data['chunks']
            self.corpus = data['corpus']
            self.doc_freqs = data['doc_freqs']
            self.idf = data['idf']
            self.doc_lens = data['doc_lens']
            self.avgdl = data['avgdl']
            self.k1 = data['k1']
            self.b = data['b']
        print(f"âœ… BM25 ç´¢å¼•å·²åŠ è½½: {len(self.chunks)} ä¸ªæ–‡æ¡£å—")

    def search(self, query: str, top_k: int = BM25_TOP_K, debug: bool = False) -> List[Dict]:
        """BM25 æ£€ç´¢ï¼ˆå¸¦è°ƒè¯•ä¿¡æ¯ï¼‰"""
        # åˆ†è¯
        query_tokens = ChineseTokenizer.tokenize(query)

        if debug:
            print(f"  ğŸ” BM25åˆ†è¯ç»“æœ: {query_tokens}")

        # è®¡ç®—æ¯ä¸ªæ–‡æ¡£çš„ BM25 åˆ†æ•°
        scores = []
        for i, doc_tokens in enumerate(self.corpus):
            score = 0
            doc_len = self.doc_lens[i]

            for token in query_tokens:
                if token not in doc_tokens:
                    continue

                # è¯é¢‘
                tf = doc_tokens.count(token)

                # IDF
                idf = self.idf.get(token, 0)

                # BM25 å…¬å¼
                numerator = tf * (self.k1 + 1)
                denominator = tf + self.k1 * (1 - self.b + self.b * (doc_len / self.avgdl))
                score += idf * (numerator / denominator)

            scores.append((i, score))

        # æ’åº
        scores.sort(key=lambda x: x[1], reverse=True)

        # è¿”å› top-k
        results = []
        for idx, score in scores[:top_k]:
            if score > 0:  # åªè¿”å›åŒ¹é…çš„æ–‡æ¡£
                chunk = self.chunks[idx].copy()
                chunk['bm25_score'] = score
                chunk['bm25_rank'] = len(results) + 1
                results.append(chunk)

        if debug:
            print(f"  âœ… BM25æ£€ç´¢å®Œæˆ: {len(results)} ä¸ªåŒ¹é…æ–‡æ¡£")

        return results


# ============================================================================
# FAISS å‘é‡ç´¢å¼•
# ============================================================================

class FaissIndex:
    """FAISS å‘é‡ç´¢å¼•"""

    def __init__(self, dimension: int = 768):
        self.dimension = dimension
        self.index = None
        self.chunks = []
        self.embeddings = None

    def build_index(self, chunks: List[Dict], embeddings: List[List[float]]):
        """æ„å»ºç´¢å¼•"""
        self.chunks = chunks
        self.embeddings = np.array(embeddings, dtype='float32')

        if self.embeddings.shape[1] != self.dimension:
            self.dimension = self.embeddings.shape[1]

        self.index = faiss.IndexFlatL2(self.dimension)
        self.index.add(self.embeddings)

        print(f"âœ… FAISS ç´¢å¼•æ„å»ºå®Œæˆ: {len(chunks)} ä¸ªæ–‡æ¡£å—")

    def save(self):
        """ä¿å­˜ç´¢å¼•"""
        faiss.write_index(self.index, f"{INDEX_PREFIX}_faiss.bin")
        with open(f"{INDEX_PREFIX}_chunks.pkl", 'wb') as f:
            pickle.dump({
                'chunks': self.chunks,
                'embeddings': self.embeddings
            }, f)
        print(f"âœ… FAISS ç´¢å¼•å·²ä¿å­˜: {INDEX_PREFIX}_faiss.bin")

    def load(self):
        """åŠ è½½ç´¢å¼•"""
        self.index = faiss.read_index(f"{INDEX_PREFIX}_faiss.bin")
        with open(f"{INDEX_PREFIX}_chunks.pkl", 'rb') as f:
            data = pickle.load(f)
            self.chunks = data['chunks']
            self.embeddings = data['embeddings']

        self.dimension = self.index.d
        print(f"âœ… FAISS ç´¢å¼•å·²åŠ è½½: {len(self.chunks)} ä¸ªæ–‡æ¡£å—")

    def search(self, query_embedding: List[float], top_k: int = VECTOR_TOP_K) -> List[Dict]:
        """å‘é‡æ£€ç´¢"""
        query_array = np.array([query_embedding], dtype='float32')
        distances, indices = self.index.search(query_array, top_k)

        results = []
        for i, (idx, dist) in enumerate(zip(indices[0], distances[0])):
            if idx < len(self.chunks):
                chunk = self.chunks[idx].copy()
                chunk['vector_score'] = float(dist)
                chunk['vector_rank'] = i + 1
                results.append(chunk)

        return results


# ============================================================================
# æ··åˆæ£€ç´¢ç»“æœèåˆï¼ˆRRFï¼‰
# ============================================================================

class HybridSearchFusion:
    """
    æ··åˆæ£€ç´¢ç»“æœèåˆ

    ä½¿ç”¨ RRF (Reciprocal Rank Fusion) ç®—æ³•èåˆå¤šç§æ£€ç´¢ç»“æœ

    RRF å…¬å¼ï¼š
    score(d) = Î£ 1/(k + rank_i(d))

    å…¶ä¸­ k æ˜¯å¸¸æ•°ï¼ˆé€šå¸¸ä¸º60ï¼‰ï¼Œrank_i æ˜¯æ–‡æ¡£åœ¨ç¬¬iç§æ£€ç´¢æ–¹æ³•ä¸­çš„æ’å
    """

    def __init__(self, k: int = 60):
        """
        å‚æ•°:
            k: RRF å¸¸æ•°ï¼ˆé»˜è®¤60ï¼‰
        """
        self.k = k

    def fuse_results(self, vector_results: List[Dict], bm25_results: List[Dict]) -> List[Dict]:
        """
        èåˆå‘é‡æ£€ç´¢å’ŒBM25æ£€ç´¢ç»“æœ

        å‚æ•°:
            vector_results: å‘é‡æ£€ç´¢ç»“æœ
            bm25_results: BM25æ£€ç´¢ç»“æœ

        è¿”å›:
            èåˆåçš„ç»“æœ
        """
        # ä½¿ç”¨å­—å…¸å­˜å‚¨èåˆåˆ†æ•° {chunk_id: score}
        fused_scores = {}
        chunk_info = {}  # å­˜å‚¨chunkä¿¡æ¯

        # å¤„ç†å‘é‡æ£€ç´¢ç»“æœ
        for result in vector_results:
            chunk_id = result.get('chunk_id', -1)
            rank = result.get('vector_rank', len(vector_results))

            # RRF åˆ†æ•°
            score = 1.0 / (self.k + rank)
            fused_scores[chunk_id] = fused_scores.get(chunk_id, 0) + score

            # ä¿å­˜ä¿¡æ¯ï¼ˆç¬¬ä¸€æ¬¡é‡åˆ°æ—¶ï¼‰
            if chunk_id not in chunk_info:
                chunk_info[chunk_id] = {
                    'content': result['content'],
                    'metadata': result['metadata'],
                    'chunk_id': chunk_id,
                    'vector_rank': rank,
                    'vector_score': result.get('vector_score', 0),
                    'bm25_rank': None,
                    'bm25_score': 0
                }

        # å¤„ç† BM25 æ£€ç´¢ç»“æœ
        for result in bm25_results:
            chunk_id = result.get('chunk_id', -1)
            rank = result.get('bm25_rank', len(bm25_results))

            # RRF åˆ†æ•°
            score = 1.0 / (self.k + rank)
            fused_scores[chunk_id] = fused_scores.get(chunk_id, 0) + score

            # æ›´æ–°ä¿¡æ¯
            if chunk_id in chunk_info:
                chunk_info[chunk_id]['bm25_rank'] = rank
                chunk_info[chunk_id]['bm25_score'] = result.get('bm25_score', 0)
            else:
                chunk_info[chunk_id] = {
                    'content': result['content'],
                    'metadata': result['metadata'],
                    'chunk_id': chunk_id,
                    'vector_rank': None,
                    'vector_score': 0,
                    'bm25_rank': rank,
                    'bm25_score': result.get('bm25_score', 0)
                }

        # æŒ‰èåˆåˆ†æ•°æ’åº
        sorted_chunks = sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)

        # æ„å»ºæœ€ç»ˆç»“æœ
        final_results = []
        for i, (chunk_id, fused_score) in enumerate(sorted_chunks):
            chunk = chunk_info[chunk_id].copy()
            chunk['fused_score'] = fused_score
            chunk['fused_rank'] = i + 1
            final_results.append(chunk)

        return final_results


# ============================================================================
# Ollama é—®ç­”ç”Ÿæˆ
# ============================================================================

class OllamaChat:
    """Ollama é—®ç­”ç”Ÿæˆå™¨"""

    def __init__(self):
        self.base_url = OLLAMA_BASE_URL
        self.model = CHAT_MODEL

    def generate_answer(self, question: str, context: str) -> str:
        """åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ"""
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸‰å›½çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹å‚è€ƒæ–‡æ¡£å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

å‚è€ƒæ–‡æ¡£ï¼š
{context}

é—®é¢˜ï¼š{question}

è¦æ±‚ï¼š
1. ç­”æ¡ˆå¿…é¡»åŸºäºå‚è€ƒæ–‡æ¡£ä¸­çš„ä¿¡æ¯
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜"æä¾›çš„æ–‡æ¡£ä¸­æ²¡æœ‰åŒ…å«è¯¥é—®é¢˜çš„ç­”æ¡ˆ"
3. ä¿æŒç­”æ¡ˆç®€æ´å‡†ç¡®

ç­”æ¡ˆï¼š"""

        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {"temperature": 0.3, "num_predict": 1000}
                },
                timeout=120
            )

            if response.status_code == 200:
                return response.json().get("response", "âš ï¸  æ— æ³•ç”Ÿæˆç­”æ¡ˆ")
            return f"âš ï¸  ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {response.status_code}"
        except Exception as e:
            return f"âš ï¸  ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {e}"


# ============================================================================
# æ··åˆæ£€ç´¢ RAG ç³»ç»Ÿ
# ============================================================================

class HybridSearchRAG:
    """æ··åˆæ£€ç´¢ RAG ç³»ç»Ÿ"""

    def __init__(self):
        self.embedder = OllamaEmbedding()
        self.chat = OllamaChat()
        self.faiss_index = FaissIndex()
        self.bm25_index = BM25Index()
        self.fusion = HybridSearchFusion()

    def build_knowledge_base(self):
        """æ„å»ºçŸ¥è¯†åº“"""
        print("\n" + "=" * 80)
        print("ğŸ“š ä¸‰å›½çŸ¥è¯†åº“æ„å»º")
        print("=" * 80)

        # åŠ è½½æ–‡æ¡£
        print("\nç¬¬ä¸€æ­¥ï¼šåŠ è½½æ–‡æ¡£")
        loader = DocumentLoader()
        documents = loader.load_directory(DOCS_DIR)

        if not documents:
            print(f"âŒ æœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£")
            return

        print(f"\nâœ… å…±åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")

        # åˆ†å‰²æ–‡æœ¬
        print("\nç¬¬äºŒæ­¥ï¼šåˆ†å‰²æ–‡æœ¬")
        splitter = TextSplitter()
        chunks = splitter.split_documents(documents)

        print(f"âœ… å…±åˆ†å‰²æˆ {len(chunks)} ä¸ªæ–‡æœ¬å—")

        # ç”Ÿæˆ embedding
        print("\nç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆ Embeddingï¼ˆç”¨äºå‘é‡æ£€ç´¢ï¼‰")
        texts = [chunk['content'] for chunk in chunks]
        embeddings = self.embedder.get_embeddings_batch(texts, batch_size=10)

        # æ„å»º FAISS ç´¢å¼•
        print("\nç¬¬å››æ­¥ï¼šæ„å»º FAISS å‘é‡ç´¢å¼•")
        self.faiss_index.build_index(chunks, embeddings)

        # æ„å»º BM25 ç´¢å¼•
        print("\nç¬¬äº”æ­¥ï¼šæ„å»º BM25 å…³é”®è¯ç´¢å¼•")
        self.bm25_index.build_index(chunks)

        # ä¿å­˜ç´¢å¼•
        print("\nç¬¬å…­æ­¥ï¼šä¿å­˜ç´¢å¼•")
        self.faiss_index.save()
        self.bm25_index.save()
        print("\nâœ¨ ä¸‰å›½çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼")

    def load_knowledge_base(self):
        """åŠ è½½çŸ¥è¯†åº“"""
        self.faiss_index.load()
        self.bm25_index.load()
        print("âœ… ä¸‰å›½çŸ¥è¯†åº“å·²åŠ è½½")

    def query(self, question: str, show_comparison: bool = True) -> Dict:
        """
        æ··åˆæ£€ç´¢æŸ¥è¯¢

        å‚æ•°:
            question: ç”¨æˆ·é—®é¢˜
            show_comparison: æ˜¯å¦æ˜¾ç¤ºä¸‰ç§æ£€ç´¢æ–¹å¼å¯¹æ¯”
        """
        print(f"\n{'=' * 80}")
        print(f"ğŸ” æŸ¥è¯¢é—®é¢˜: {question}")
        print(f"{'=' * 80}")

        # 1ï¸âƒ£ å‘é‡æ£€ç´¢
        print(f"\nğŸ“Š ç¬¬ä¸€æ­¥ï¼šå‘é‡æ£€ç´¢ï¼ˆè¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰")
        query_embedding = self.embedder.get_embedding(question)
        if not query_embedding:
            return {"error": "æ— æ³•ç”ŸæˆæŸ¥è¯¢å‘é‡"}

        vector_results = self.faiss_index.search(query_embedding, top_k=VECTOR_TOP_K)
        print(f"âœ… å‘é‡æ£€ç´¢å®Œæˆï¼Œå¬å› {len(vector_results)} ä¸ªæ–‡æ¡£")

        # 2ï¸âƒ£ BM25 æ£€ç´¢
        print(f"\nğŸ” ç¬¬äºŒæ­¥ï¼šBM25 æ£€ç´¢ï¼ˆå…³é”®è¯åŒ¹é…ï¼‰")
        bm25_results = self.bm25_index.search(question, top_k=BM25_TOP_K, debug=True)
        print(f"âœ… BM25 æ£€ç´¢å®Œæˆï¼Œå¬å› {len(bm25_results)} ä¸ªæ–‡æ¡£")

        # 3ï¸âƒ£ ç»“æœèåˆ
        print(f"\nğŸ”„ ç¬¬ä¸‰æ­¥ï¼šç»“æœèåˆï¼ˆRRF ç®—æ³•ï¼‰")
        fused_results = self.fusion.fuse_results(vector_results, bm25_results)
        print(f"âœ… èåˆå®Œæˆ")

        # 4ï¸âƒ£ æ˜¾ç¤ºå¯¹æ¯”
        if show_comparison:
            self._show_comparison(vector_results, bm25_results, fused_results)

        # 5ï¸âƒ£ æœ€ç»ˆç»“æœ
        final_results = fused_results[:FINAL_TOP_K]

        # 6ï¸âƒ£ ç»„è£…ä¸Šä¸‹æ–‡
        context = "\n\n".join([
            f"ã€æ¥æº: {r['metadata'][:50]}ã€‘\n{r['content'][:400]}"
            for r in final_results
        ])

        # 7ï¸âƒ£ ç”Ÿæˆç­”æ¡ˆ
        print("\nğŸ’­ æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...")
        answer = self.chat.generate_answer(question, context)

        return {
            "answer": answer,
            "sources": final_results,
            "vector_results": vector_results[:FINAL_TOP_K],
            "bm25_results": bm25_results[:FINAL_TOP_K],
            "query": question
        }

    def _show_comparison(self, vector_results: List[Dict], bm25_results: List[Dict], fused_results: List[Dict]):
        """æ˜¾ç¤ºä¸‰ç§æ£€ç´¢æ–¹å¼çš„å¯¹æ¯”"""
        print(f"\n{'=' * 80}")
        print("ğŸ“Š ä¸‰ç§æ£€ç´¢æ–¹å¼å¯¹æ¯”ï¼ˆTop-5ï¼‰")
        print('=' * 80)

        print("\nã€å‘é‡æ£€ç´¢ Top-5ã€‘ï¼ˆè¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰")
        for i, r in enumerate(vector_results[:5]):
            rank = r.get('vector_rank', i+1)
            score = r.get('vector_score', 0)
            print(f"  [{i+1}] #{rank:2d} {r['metadata'][:45]:45s} (L2: {score:.4f})")

        print("\nã€BM25 æ£€ç´¢ Top-5ã€‘ï¼ˆå…³é”®è¯åŒ¹é…ï¼‰")
        for i, r in enumerate(bm25_results[:5]):
            rank = r.get('bm25_rank', i+1)
            score = r.get('bm25_score', 0)
            print(f"  [{i+1}] #{rank:2d} {r['metadata'][:45]:45s} (BM25: {score:.2f})")

        print("\nã€æ··åˆæ£€ç´¢ Top-5ã€‘ï¼ˆèåˆç»“æœï¼‰â­")
        for i, r in enumerate(fused_results[:5]):
            v_rank = r.get('vector_rank', '-')
            b_rank = r.get('bm25_rank', '-')
            score = r.get('fused_score', 0)

            v_str = f"#{v_rank}" if v_rank != '-' else " - "
            b_str = f"#{b_rank}" if b_rank != '-' else " - "

            print(f"  [{i+1}] {r['metadata'][:45]:45s}")
            print(f"      å‘é‡æ’å: {v_str:3s} | BM25æ’å: {b_str:3s} | èåˆåˆ†æ•°: {score:.4f}")


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================

def main():
    """ä¸»ç¨‹åº"""

    print("=" * 80)
    print("ğŸ¤– ä¸‰å›½çŸ¥è¯†åº“ + æ··åˆæ£€ç´¢ç³»ç»Ÿ")
    print("=" * 80)
    print("\nğŸ“š åŸºäºã€Šä¸‰å›½æ¼”ä¹‰ã€‹1.7MB æ–‡æœ¬")
    print("ğŸ¯ æ¼”ç¤º RAG é«˜æ•ˆå¬å›æ–¹æ³•ï¼šæ··åˆæ£€ç´¢ï¼ˆHybrid Searchï¼‰")
    print("\nğŸ” æ··åˆæ£€ç´¢ = å‘é‡æ£€ç´¢ï¼ˆFAISSï¼‰+ BM25 + RRF èåˆ")

    # æ£€æŸ¥ Ollama
    try:
        response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5)
        if response.status_code != 200:
            print("\nâŒ Ollama æœªè¿è¡Œ")
            return
    except:
        print("\nâŒ æ— æ³•è¿æ¥ Ollama")
        return

    # åˆ›å»ºç³»ç»Ÿ
    try:
        rag = HybridSearchRAG()
    except Exception as e:
        print(f"\nâŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        print("\nğŸ’¡ è¯·ç¡®ä¿å·²ä¸‹è½½ embedding æ¨¡å‹: ollama pull nomic-embed-text")
        return

    # æ„å»ºæˆ–åŠ è½½çŸ¥è¯†åº“
    if Path(f"{INDEX_PREFIX}_faiss.bin").exists() and Path(f"{INDEX_PREFIX}_bm25.pkl").exists():
        print(f"\næ£€æµ‹åˆ°å·²æœ‰ç´¢å¼•ï¼Œæ˜¯å¦é‡æ–°æ„å»ºï¼Ÿ(y/n): ", end="")
        try:
            choice = input().strip().lower()
            if choice == 'y':
                rag.build_knowledge_base()
            else:
                rag.load_knowledge_base()
        except:
            rag.load_knowledge_base()
    else:
        rag.build_knowledge_base()

    # äº¤äº’å¼é—®ç­”
    print("\n" + "=" * 80)
    print("ğŸ’¬ å¼€å§‹é—®ç­”ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰")
    print("=" * 80)

    print("\nğŸ’¡ ç¤ºä¾‹é—®é¢˜ï¼ˆè§‚å¯Ÿæ··åˆæ£€ç´¢çš„æ•ˆæœï¼‰:")
    print("  - è¯¸è‘›äº®çš„æ‰‡å­ï¼ˆå…³é”®è¯+è¯­ä¹‰ï¼‰")
    print("  - èµ¤å£ä¹‹æˆ˜çš„èƒœåˆ©è€…")
    print("  - å…³ç¾½çš„æ­¦å™¨ï¼ˆé’é¾™åƒæœˆåˆ€ï¼‰")
    print("  - åˆ˜å¤‡çš„ä¸‰é¡¾èŒ…åº")

    while True:
        print("\n" + "â”€" * 80)
        try:
            question = input("â“ ä½ çš„é—®é¢˜: ").strip()

            if not question:
                continue

            if question.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ å†è§ï¼")
                break

            result = rag.query(question, show_comparison=True)

            if "error" in result:
                print(f"\nâŒ {result['error']}")
                continue

            print("\n" + "â”€" * 80)
            print("ğŸ“– ç­”æ¡ˆ:")
            print("â”€" * 80)
            print(result['answer'])
            print("â”€" * 80)

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ å‡ºé”™äº†: {e}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    main()
