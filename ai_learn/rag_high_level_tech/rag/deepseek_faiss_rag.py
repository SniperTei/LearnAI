"""
DeepSeek + Faiss æœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢ç³»ç»Ÿ
=====================================

åŠŸèƒ½ï¼š
1. åŠ è½½æœ¬åœ°æ–‡æ¡£ï¼ˆtxt, pdf, docxï¼‰
2. ä½¿ç”¨ DeepSeek API ç”Ÿæˆ Embedding
3. ä½¿ç”¨ Faiss æ„å»ºå‘é‡ç´¢å¼•
4. æ£€ç´¢ç›¸å…³æ–‡æ¡£å¹¶ç”¨ DeepSeek ç”Ÿæˆç­”æ¡ˆ

ä½œè€…: Claude Code Assistant
æ—¥æœŸ: 2026-01-27
"""

import os
import json
import pickle
from typing import List, Dict, Tuple
from pathlib import Path
import re

import numpy as np
import faiss
from dotenv import load_dotenv
from openai import OpenAI

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


# ============================================================================
# 1. æ–‡æ¡£åŠ è½½å’Œé¢„å¤„ç†
# ============================================================================

class DocumentLoader:
    """æ–‡æ¡£åŠ è½½å™¨ - æ”¯æŒå¤šç§æ ¼å¼"""

    @staticmethod
    def load_txt(file_path: str) -> str:
        """åŠ è½½ TXT æ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()

    @staticmethod
    def load_pdf(file_path: str) -> str:
        """åŠ è½½ PDF æ–‡ä»¶"""
        try:
            import PyPDF2
            with open(file_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                text = ""
                for page in reader.pages:
                    text += page.extract_text()
                return text
        except ImportError:
            print("âš ï¸  è¯·å®‰è£… PyPDF2: pip install PyPDF2")
            return ""

    @staticmethod
    def load_docx(file_path: str) -> str:
        """åŠ è½½ DOCX æ–‡ä»¶"""
        try:
            import docx
            doc = docx.Document(file_path)
            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            return text
        except ImportError:
            print("âš ï¸  è¯·å®‰è£… python-docx: pip install python-docx")
            return ""

    @staticmethod
    def load_file(file_path: str) -> str:
        """æ ¹æ®æ–‡ä»¶æ‰©å±•åè‡ªåŠ¨åŠ è½½"""
        ext = Path(file_path).suffix.lower()

        loaders = {
            '.txt': DocumentLoader.load_txt,
            '.pdf': DocumentLoader.load_pdf,
            '.docx': DocumentLoader.load_docx,
        }

        loader = loaders.get(ext)
        if loader:
            return loader(file_path)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {ext}")

    @staticmethod
    def load_directory(directory: str) -> List[Tuple[str, str]]:
        """
        åŠ è½½ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡æ¡£

        è¿”å›: [(æ–‡ä»¶å, æ–‡æ¡£å†…å®¹), ...]
        """
        documents = []
        path = Path(directory)

        supported_exts = ['.txt', '.pdf', '.docx']

        for file_path in path.rglob('*'):
            if file_path.suffix.lower() in supported_exts:
                try:
                    content = DocumentLoader.load_file(str(file_path))
                    if content.strip():
                        documents.append((file_path.name, content))
                        print(f"âœ… å·²åŠ è½½: {file_path.name}")
                except Exception as e:
                    print(f"âŒ åŠ è½½å¤±è´¥ {file_path.name}: {e}")

        return documents


class TextSplitter:
    """æ–‡æœ¬åˆ†å‰²å™¨ - å°†æ–‡æ¡£åˆ‡åˆ†æˆå°å—"""

    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):
        """
        å‚æ•°:
            chunk_size: æ¯å—çš„å­—ç¬¦æ•°
            chunk_overlap: å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

    def split_text(self, text: str, metadata: str = "") -> List[Dict]:
        """
        åˆ†å‰²æ–‡æœ¬

        è¿”å›: [{"content": æ–‡æœ¬å—, "metadata": å…ƒæ•°æ®}, ...]
        """
        chunks = []

        # æ¸…ç†æ–‡æœ¬
        text = re.sub(r'\n+', '\n', text)  # åˆå¹¶å¤šä½™æ¢è¡Œ
        text = text.strip()

        # æŒ‰æ®µè½åˆ†å‰²ï¼ˆå¦‚æœæ®µè½å¤ªé•¿å†æŒ‰å­—ç¬¦åˆ†å‰²ï¼‰
        paragraphs = text.split('\n\n')

        current_chunk = ""
        chunk_id = 0

        for para in paragraphs:
            para = para.strip()
            if not para:
                continue

            # å¦‚æœå½“å‰å—åŠ ä¸Šæ–°æ®µè½ä¸è¶…è¿‡é™åˆ¶
            if len(current_chunk) + len(para) + 2 <= self.chunk_size:
                current_chunk += para + "\n\n"
            else:
                # ä¿å­˜å½“å‰å—
                if current_chunk.strip():
                    chunks.append({
                        "content": current_chunk.strip(),
                        "metadata": metadata,
                        "chunk_id": chunk_id
                    })
                    chunk_id += 1

                # å¼€å§‹æ–°å—ï¼ˆå¦‚æœæœ‰é‡å ï¼Œä¿ç•™éƒ¨åˆ†å†…å®¹ï¼‰
                if self.chunk_overlap > 0 and current_chunk:
                    overlap_text = current_chunk[-self.chunk_overlap:]
                    current_chunk = overlap_text + para + "\n\n"
                else:
                    current_chunk = para + "\n\n"

        # ä¿å­˜æœ€åä¸€å—
        if current_chunk.strip():
            chunks.append({
                "content": current_chunk.strip(),
                "metadata": metadata,
                "chunk_id": chunk_id
            })

        return chunks

    def split_documents(self, documents: List[Tuple[str, str]]) -> List[Dict]:
        """
        åˆ†å‰²å¤šä¸ªæ–‡æ¡£

        å‚æ•°:
            documents: [(æ–‡ä»¶å, å†…å®¹), ...]

        è¿”å›: [æ–‡æœ¬å—å­—å…¸, ...]
        """
        all_chunks = []

        for filename, content in documents:
            chunks = self.split_text(content, metadata=filename)
            all_chunks.extend(chunks)

        return all_chunks


# ============================================================================
# 2. DeepSeek Embedding
# ============================================================================

class DeepSeekEmbedding:
    """DeepSeek Embedding ç”Ÿæˆå™¨"""

    def __init__(self, api_key: str = None, base_url: str = None):
        """
        åˆå§‹åŒ– DeepSeek å®¢æˆ·ç«¯

        DeepSeek API å…¼å®¹ OpenAI SDK
        """
        self.api_key = api_key or os.getenv("DEEPSEEK_API_KEY")
        self.base_url = base_url or os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com")

        if not self.api_key:
            raise ValueError("è¯·è®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")

        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )

    def get_embedding(self, text: str) -> List[float]:
        """
        è·å–æ–‡æœ¬çš„ Embedding

        æ³¨æ„ï¼šDeepSeek å¯èƒ½éœ€è¦é€šè¿‡ chat æ¨¡å‹ç”Ÿæˆ embedding
        è¿™é‡Œæä¾›å…¼å®¹æ¥å£
        """
        try:
            # å°è¯•ä½¿ç”¨ embeddings endpointï¼ˆå¦‚æœ DeepSeek æ”¯æŒï¼‰
            response = self.client.embeddings.create(
                model="text-embedding-ada-002",  # æˆ– DeepSeek çš„ embedding æ¨¡å‹
                input=text
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"âš ï¸  DeepSeek embedding è°ƒç”¨å¤±è´¥: {e}")
            print("ğŸ’¡ å»ºè®®ï¼šä½¿ç”¨ sentence-transformers ä½œä¸ºæ›¿ä»£")
            # è¿”å›é›¶å‘é‡ä½œä¸º fallback
            return [0.0] * 1536

    def get_embeddings_batch(self, texts: List[str], batch_size: int = 10) -> List[List[float]]:
        """
        æ‰¹é‡è·å– Embedding
        """
        embeddings = []

        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            for text in batch:
                emb = self.get_embedding(text)
                embeddings.append(emb)
                print(f"âœ… ç”Ÿæˆ embedding {len(embeddings)}/{len(texts)}")

        return embeddings


# ============================================================================
# 3. Faiss ç´¢å¼•
# ============================================================================

class FaissIndex:
    """Faiss å‘é‡ç´¢å¼•ç®¡ç†å™¨"""

    def __init__(self, dimension: int = 1536):
        """
        å‚æ•°:
            dimension: å‘é‡ç»´åº¦ï¼ˆ1536 for OpenAI ada-002ï¼‰
        """
        self.dimension = dimension
        self.index = None
        self.chunks = []  # å­˜å‚¨æ–‡æœ¬å—

    def build_index(self, chunks: List[Dict], embeddings: List[List[float]]):
        """
        æ„å»º Faiss ç´¢å¼•

        å‚æ•°:
            chunks: æ–‡æœ¬å—åˆ—è¡¨
            embeddings: å¯¹åº”çš„ embedding åˆ—è¡¨
        """
        self.chunks = chunks

        # è½¬æ¢ä¸º numpy æ•°ç»„
        embeddings_array = np.array(embeddings, dtype='float32')

        # ç¡®ä¿ç»´åº¦æ­£ç¡®
        if embeddings_array.shape[1] != self.dimension:
            print(f"âš ï¸  å‘é‡ç»´åº¦ä¸åŒ¹é…: æœŸæœ› {self.dimension}, å®é™… {embeddings_array.shape[1]}")
            self.dimension = embeddings_array.shape[1]

        # åˆ›å»ºç´¢å¼•ï¼ˆä½¿ç”¨ L2 è·ç¦»ï¼‰
        self.index = faiss.IndexFlatL2(self.dimension)

        # æ·»åŠ å‘é‡
        self.index.add(embeddings_array)

        print(f"âœ… Faiss ç´¢å¼•æ„å»ºå®Œæˆ: {len(chunks)} ä¸ªæ–‡æ¡£å—")

    def save(self, index_path: str = "faiss_index.bin", data_path: str = "chunks.pkl"):
        """
        ä¿å­˜ç´¢å¼•å’Œæ•°æ®
        """
        if self.index is None:
            raise ValueError("ç´¢å¼•æœªæ„å»ºï¼Œæ— æ³•ä¿å­˜")

        # ä¿å­˜ Faiss ç´¢å¼•
        faiss.write_index(self.index, index_path)

        # ä¿å­˜æ–‡æœ¬å—æ•°æ®
        with open(data_path, 'wb') as f:
            pickle.dump(self.chunks, f)

        print(f"âœ… ç´¢å¼•å·²ä¿å­˜: {index_path}, {data_path}")

    def load(self, index_path: str = "faiss_index.bin", data_path: str = "chunks.pkl"):
        """
        åŠ è½½ç´¢å¼•å’Œæ•°æ®
        """
        # åŠ è½½ Faiss ç´¢å¼•
        self.index = faiss.read_index(index_path)

        # åŠ è½½æ–‡æœ¬å—æ•°æ®
        with open(data_path, 'rb') as f:
            self.chunks = pickle.load(f)

        self.dimension = self.index.d

        print(f"âœ… ç´¢å¼•å·²åŠ è½½: {len(self.chunks)} ä¸ªæ–‡æ¡£å—")

    def search(self, query_embedding: List[float], top_k: int = 5) -> List[Dict]:
        """
        æœç´¢æœ€ç›¸ä¼¼çš„æ–‡æ¡£å—

        è¿”å›: [{"content": æ–‡æœ¬, "metadata": å…ƒæ•°æ®, "score": ç›¸ä¼¼åº¦åˆ†æ•°}, ...]
        """
        if self.index is None:
            raise ValueError("ç´¢å¼•æœªæ„å»ºï¼Œè¯·å…ˆæ„å»ºæˆ–åŠ è½½ç´¢å¼•")

        # è½¬æ¢æŸ¥è¯¢å‘é‡
        query_array = np.array([query_embedding], dtype='float32')

        # æœç´¢
        distances, indices = self.index.search(query_array, top_k)

        # æ•´ç†ç»“æœ
        results = []
        for i, (idx, dist) in enumerate(zip(indices[0], distances[0])):
            if idx < len(self.chunks):
                chunk = self.chunks[idx].copy()
                chunk['score'] = float(dist)
                results.append(chunk)

        return results


# ============================================================================
# 4. DeepSeek é—®ç­”ç”Ÿæˆ
# ============================================================================

class DeepSeekChat:
    """DeepSeek é—®ç­”ç”Ÿæˆå™¨"""

    def __init__(self, api_key: str = None, base_url: str = None):
        self.api_key = api_key or os.getenv("DEEPSEEK_API_KEY")
        self.base_url = base_url or os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com")

        if not self.api_key:
            raise ValueError("è¯·è®¾ç½® DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡")

        self.client = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )

    def generate_answer(self, question: str, context: str) -> str:
        """
        åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ
        """
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹å‚è€ƒæ–‡æ¡£å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

å‚è€ƒæ–‡æ¡£ï¼š
{context}

é—®é¢˜ï¼š{question}

è¦æ±‚ï¼š
1. ç­”æ¡ˆå¿…é¡»åŸºäºå‚è€ƒæ–‡æ¡£ä¸­çš„ä¿¡æ¯
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜
3. ä¿æŒç­”æ¡ˆç®€æ´å‡†ç¡®

ç­”æ¡ˆï¼š"""

        try:
            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=1000
            )

            return response.choices[0].message.content

        except Exception as e:
            return f"âš ï¸  ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {e}"


# ============================================================================
# 5. å®Œæ•´çš„ RAG ç³»ç»Ÿ
# ============================================================================

class RAGSystem:
    """å®Œæ•´çš„ RAG ç³»ç»Ÿ"""

    def __init__(self):
        self.embedder = DeepSeekEmbedding()
        self.chat = DeepSeekChat()
        self.index = FaissIndex()

    def build_knowledge_base(self, documents_dir: str):
        """
        æ„å»ºçŸ¥è¯†åº“

        å‚æ•°:
            documents_dir: æ–‡æ¡£ç›®å½•è·¯å¾„
        """
        print("\n" + "=" * 80)
        print("ğŸ“š ç¬¬ä¸€æ­¥ï¼šåŠ è½½æ–‡æ¡£")
        print("=" * 80)

        # 1. åŠ è½½æ–‡æ¡£
        loader = DocumentLoader()
        documents = loader.load_directory(documents_dir)

        if not documents:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£")
            return

        print(f"\nâœ… å…±åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")

        # 2. åˆ†å‰²æ–‡æœ¬
        print("\n" + "=" * 80)
        print("âœ‚ï¸  ç¬¬äºŒæ­¥ï¼šåˆ†å‰²æ–‡æœ¬")
        print("=" * 80)

        splitter = TextSplitter(chunk_size=500, chunk_overlap=50)
        chunks = splitter.split_documents(documents)

        print(f"âœ… å…±åˆ†å‰²æˆ {len(chunks)} ä¸ªæ–‡æœ¬å—")

        # 3. ç”Ÿæˆ Embedding
        print("\n" + "=" * 80)
        print("ğŸ”„ ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆ Embedding")
        print("=" * 80)

        texts = [chunk['content'] for chunk in chunks]
        embeddings = self.embedder.get_embeddings_batch(texts, batch_size=10)

        # 4. æ„å»º Faiss ç´¢å¼•
        print("\n" + "=" * 80)
        print("ğŸ”¨ ç¬¬å››æ­¥ï¼šæ„å»º Faiss ç´¢å¼•")
        print("=" * 80)

        self.index.build_index(chunks, embeddings)

        # 5. ä¿å­˜ç´¢å¼•
        print("\n" + "=" * 80)
        print("ğŸ’¾ ç¬¬äº”æ­¥ï¼šä¿å­˜ç´¢å¼•")
        print("=" * 80)

        self.index.save()
        print("\nâœ¨ çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼")

    def load_knowledge_base(self):
        """åŠ è½½å·²æ„å»ºçš„çŸ¥è¯†åº“"""
        self.index.load()
        print("âœ… çŸ¥è¯†åº“å·²åŠ è½½")

    def query(self, question: str, top_k: int = 3) -> Dict:
        """
        æŸ¥è¯¢çŸ¥è¯†åº“

        è¿”å›: {"answer": ç­”æ¡ˆ, "sources": [æ¥æºæ–‡æ¡£], "query": é—®é¢˜}
        """
        print(f"\nğŸ” æŸ¥è¯¢é—®é¢˜: {question}")

        # 1. ç”Ÿæˆé—®é¢˜ embedding
        query_embedding = self.embedder.get_embedding(question)

        # 2. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        results = self.index.search(query_embedding, top_k=top_k)

        print(f"ğŸ“š æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£:")

        for i, result in enumerate(results):
            print(f"\n  [{i+1}] {result['metadata']} (åˆ†æ•°: {result['score']:.4f})")
            print(f"      {result['content'][:100]}...")

        # 3. ç»„è£…ä¸Šä¸‹æ–‡
        context = "\n\n".join([
            f"ã€æ¥æº: {r['metadata']}ã€‘\n{r['content']}"
            for r in results
        ])

        # 4. ç”Ÿæˆç­”æ¡ˆ
        print("\nğŸ’­ æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...")
        answer = self.chat.generate_answer(question, context)

        return {
            "answer": answer,
            "sources": results,
            "query": question
        }


# ============================================================================
# 6. ä¸»ç¨‹åº
# ============================================================================

def main():
    """ä¸»ç¨‹åºæ¼”ç¤º"""

    print("=" * 80)
    print("ğŸ¤– DeepSeek + Faiss æœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢ç³»ç»Ÿ")
    print("=" * 80)

    # æ£€æŸ¥ API Key
    if not os.getenv("DEEPSEEK_API_KEY"):
        print("\nâŒ é”™è¯¯ï¼šæœªè®¾ç½® DEEPSEEK_API_KEY")
        print("\nè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤é…ç½®ï¼š")
        print("1. å¤åˆ¶ .env.example ä¸º .env")
        print("2. åœ¨ .env ä¸­è®¾ç½®ä½ çš„ DeepSeek API Key")
        print("\nè·å– API Key: https://platform.deepseek.com/")
        return

    # åˆ›å»º RAG ç³»ç»Ÿ
    rag = RAGSystem()

    # åˆ›å»ºç¤ºä¾‹æ–‡æ¡£ç›®å½•
    docs_dir = "knowledge_base"
    Path(docs_dir).mkdir(exist_ok=True)

    # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ–‡æ¡£
    if not list(Path(docs_dir).rglob('*')):
        print(f"\nğŸ“ åœ¨ {docs_dir}/ ç›®å½•ä¸­æ·»åŠ ä½ çš„æ–‡æ¡£ï¼ˆtxt, pdf, docxï¼‰")
        print("ç„¶åé‡æ–°è¿è¡Œç¨‹åº")
        return

    # æ„å»ºæˆ–åŠ è½½çŸ¥è¯†åº“
    if Path("faiss_index.bin").exists():
        print("\næ£€æµ‹åˆ°å·²æœ‰ç´¢å¼•ï¼Œæ˜¯å¦é‡æ–°æ„å»ºï¼Ÿ(y/n): ", end="")
        choice = input().strip().lower()

        if choice == 'y':
            rag.build_knowledge_base(docs_dir)
        else:
            rag.load_knowledge_base()
    else:
        rag.build_knowledge_base(docs_dir)

    # äº¤äº’å¼é—®ç­”
    print("\n" + "=" * 80)
    print("ğŸ’¬ å¼€å§‹é—®ç­”ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰")
    print("=" * 80)

    while True:
        print("\n" + "â”€" * 80)
        question = input("â“ ä½ çš„é—®é¢˜: ").strip()

        if not question:
            continue

        if question.lower() in ['quit', 'exit', 'q']:
            print("ğŸ‘‹ å†è§ï¼")
            break

        # æŸ¥è¯¢
        result = rag.query(question, top_k=3)

        # æ˜¾ç¤ºç­”æ¡ˆ
        print("\n" + "â”€" * 80)
        print("ğŸ“– ç­”æ¡ˆ:")
        print("â”€" * 80)
        print(result['answer'])
        print("â”€" * 80)


if __name__ == "__main__":
    main()
