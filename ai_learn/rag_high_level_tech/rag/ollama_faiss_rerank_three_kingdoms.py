"""
Ollama + Faiss + Rerank ä¸‰å›½çŸ¥è¯†åº“æ£€ç´¢ç³»ç»Ÿ
========================================

ä½¿ç”¨æœ¬åœ° Ollama æ¨¡å‹ + Faiss + Rerank æ„å»ºçš„ä¸‰å›½çŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿã€‚
æ¼”ç¤º RAG é«˜æ•ˆå¬å›æ–¹æ³•ï¼šé‡æ’åºï¼ˆRerankingï¼‰åœ¨å¤§å‹æ•°æ®é›†ä¸Šçš„æ•ˆæœ

ç‰¹ç‚¹ï¼š
- ä¸“é—¨ç”¨äºä¸‰å›½æ¼”ä¹‰ï¼ˆ1.7MBï¼Œå¤§é‡äººç‰©å’Œäº‹ä»¶ï¼‰
- ä¼˜åŒ–çš„å‚æ•°è®¾ç½®
- æ¸…æ™°å±•ç¤º Rerank çš„æ•ˆæœ

ä½œè€…: Claude Code Assistant
æ—¥æœŸ: 2026-01-27
"""

import os
import sys
import json
import pickle
from typing import List, Dict
from pathlib import Path
import re
import requests

import numpy as np
import faiss

# ============================================================================
# é…ç½®
# ============================================================================

OLLAMA_BASE_URL = "http://localhost:11434"
EMBEDDING_MODEL = "nomic-embed-text"
CHAT_MODEL = "deepseek-r1:1.5b"

# ä¸‰å›½çŸ¥è¯†åº“ä¸“ç”¨é…ç½®
DOCS_DIR = "knowledge_threekingdoms"
INDEX_PREFIX = "threekingdoms"  # ç´¢å¼•æ–‡ä»¶å‰ç¼€
CHUNK_SIZE = 800          # å¤§æ–‡æ¡£ç”¨æ›´å¤§çš„å—
CHUNK_OVERLAP = 100       # æ›´å¤§çš„é‡å 
COARSE_TOP_K = 100        # ç²—æ’å¬å›100ä¸ªï¼ˆæ•°æ®é‡å¤§ï¼‰
FINAL_TOP_K = 5           # æœ€ç»ˆè¿”å›5ä¸ª

# ============================================================================
# å¯¼å…¥åŸæ¥çš„ç±»ï¼ˆè¿™é‡Œç®€åŒ–é‡å†™ï¼Œé¿å…ä»£ç å¤ªé•¿ï¼‰
# ============================================================================

class DocumentLoader:
    """æ–‡æ¡£åŠ è½½å™¨"""
    @staticmethod
    def load_txt(file_path: str) -> str:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()

    @staticmethod
    def load_directory(directory: str) -> List[tuple]:
        documents = []
        path = Path(directory)
        for file_path in path.rglob('*.txt'):
            try:
                content = DocumentLoader.load_txt(str(file_path))
                if content.strip():
                    documents.append((file_path.name, content))
                    print(f"âœ… å·²åŠ è½½: {file_path.name} ({len(content):,} å­—ç¬¦)")
            except Exception as e:
                print(f"âŒ åŠ è½½å¤±è´¥ {file_path.name}: {e}")
        return documents

class TextSplitter:
    """æ–‡æœ¬åˆ†å‰²å™¨"""
    def __init__(self, chunk_size: int = CHUNK_SIZE, chunk_overlap: int = CHUNK_OVERLAP):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

    def split_text(self, text: str, metadata: str = "") -> List[Dict]:
        chunks = []
        text = re.sub(r'\n+', '\n', text).strip()
        paragraphs = text.split('\n\n')

        current_chunk = ""
        chunk_id = 0

        for para in paragraphs:
            para = para.strip()
            if not para:
                continue

            if len(current_chunk) + len(para) + 2 <= self.chunk_size:
                current_chunk += para + "\n\n"
            else:
                if current_chunk.strip():
                    chunks.append({
                        "content": current_chunk.strip(),
                        "metadata": metadata,
                        "chunk_id": chunk_id
                    })
                    chunk_id += 1

                if self.chunk_overlap > 0 and current_chunk:
                    overlap_text = current_chunk[-self.chunk_overlap:]
                    current_chunk = overlap_text + para + "\n\n"
                else:
                    current_chunk = para + "\n\n"

        if current_chunk.strip():
            chunks.append({
                "content": current_chunk.strip(),
                "metadata": metadata,
                "chunk_id": chunk_id
            })

        return chunks

    def split_documents(self, documents: List[tuple]) -> List[Dict]:
        all_chunks = []
        for filename, content in documents:
            chunks = self.split_text(content, metadata=filename)
            all_chunks.extend(chunks)
        return all_chunks

class OllamaEmbedding:
    """Ollama Embedding ç”Ÿæˆå™¨"""
    def __init__(self):
        self.base_url = OLLAMA_BASE_URL
        self.model = EMBEDDING_MODEL
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code != 200:
                raise Exception("Ollama æœªè¿è¡Œ")
            print(f"âœ… Ollama è¿æ¥æˆåŠŸ")
        except Exception as e:
            raise Exception(f"æ— æ³•è¿æ¥åˆ° Ollama: {e}")

    def get_embedding(self, text: str) -> List[float]:
        try:
            response = requests.post(
                f"{self.base_url}/api/embeddings",
                json={"model": self.model, "prompt": text},
                timeout=60
            )
            if response.status_code == 200:
                return response.json().get("embedding", [])
            return []
        except:
            return []

    def get_embeddings_batch(self, texts: List[str], batch_size: int = 20) -> List[List[float]]:
        """æ‰¹é‡è·å– Embedding"""
        embeddings = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            for text in batch:
                emb = self.get_embedding(text)
                if emb:
                    embeddings.append(emb)
                    print(f"âœ… [{len(embeddings)}/{len(texts)}] ç”Ÿæˆ embedding ({len(text):,} å­—ç¬¦)")
                else:
                    embeddings.append([0.0] * 768)
        return embeddings

class FaissIndex:
    """Faiss å‘é‡ç´¢å¼•"""
    def __init__(self, dimension: int = 768):
        self.dimension = dimension
        self.index = None
        self.chunks = []
        self.embeddings = None

    def build_index(self, chunks: List[Dict], embeddings: List[List[float]]):
        self.chunks = chunks
        self.embeddings = np.array(embeddings, dtype='float32')

        if self.embeddings.shape[1] != self.dimension:
            self.dimension = self.embeddings.shape[1]

        self.index = faiss.IndexFlatL2(self.dimension)
        self.index.add(self.embeddings)

        print(f"âœ… Faiss ç´¢å¼•æ„å»ºå®Œæˆ: {len(chunks)} ä¸ªæ–‡æ¡£å—")

    def save(self):
        """ä¿å­˜ç´¢å¼•"""
        faiss.write_index(self.index, f"{INDEX_PREFIX}_faiss.bin")
        with open(f"{INDEX_PREFIX}_chunks.pkl", 'wb') as f:
            pickle.dump({
                'chunks': self.chunks,
                'embeddings': self.embeddings
            }, f)
        print(f"âœ… ç´¢å¼•å·²ä¿å­˜: {INDEX_PREFIX}_*.bin")

    def load(self):
        """åŠ è½½ç´¢å¼•"""
        self.index = faiss.read_index(f"{INDEX_PREFIX}_faiss.bin")
        with open(f"{INDEX_PREFIX}_chunks.pkl", 'rb') as f:
            data = pickle.load(f)
            self.chunks = data['chunks']
            self.embeddings = data['embeddings']
        self.dimension = self.index.d
        print(f"âœ… ç´¢å¼•å·²åŠ è½½: {len(self.chunks)} ä¸ªæ–‡æ¡£å—")

    def search(self, query_embedding: List[float], top_k: int = COARSE_TOP_K) -> List[Dict]:
        """æœç´¢æ–‡æ¡£"""
        query_array = np.array([query_embedding], dtype='float32')
        distances, indices = self.index.search(query_array, top_k)

        results = []
        for i, (idx, dist) in enumerate(zip(indices[0], distances[0])):
            if idx < len(self.chunks):
                chunk = self.chunks[idx].copy()
                chunk['coarse_score'] = float(dist)
                chunk['embedding'] = self.embeddings[idx]
                results.append(chunk)

        return results

class Reranker:
    """Rerank é‡æ’åºå™¨"""
    def __init__(self, method: str = "vector"):
        self.method = method

    def rerank(self, query: str, query_embedding: List[float], documents: List[Dict]) -> List[Dict]:
        """é‡æ’åº"""
        if self.method == "vector":
            print("ğŸ”„ ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦è¿›è¡Œ rerank...")
            query_vec = np.array(query_embedding).reshape(1, -1)

            for doc in documents:
                doc_vec = doc['embedding'].reshape(1, -1)
                cosine_sim = np.dot(query_vec, doc_vec.T) / (
                    np.linalg.norm(query_vec) * np.linalg.norm(doc_vec)
                )

                l2_dist = doc['coarse_score']
                l2_sim = 1 / (1 + l2_dist)
                doc['rerank_score'] = 0.7 * cosine_sim[0][0] + 0.3 * l2_sim
                doc['cosine_sim'] = cosine_sim[0][0]

            return sorted(documents, key=lambda x: x['rerank_score'], reverse=True)

        elif self.method == "keyword":
            print("ğŸ”„ ä½¿ç”¨å…³é”®è¯åŒ¹é…è¿›è¡Œ rerank...")
            query_keywords = set(query.lower().split())

            for doc in documents:
                content = doc['content'].lower()
                keyword_matches = sum(content.count(kw) for kw in query_keywords)
                l2_sim = 1 / (1 + doc['coarse_score'])
                doc['rerank_score'] = l2_sim * (1 + keyword_matches * 0.1)
                doc['keyword_matches'] = keyword_matches

            return sorted(documents, key=lambda x: x['rerank_score'], reverse=True)

class OllamaChat:
    """Ollama é—®ç­”ç”Ÿæˆå™¨"""
    def __init__(self):
        self.base_url = OLLAMA_BASE_URL
        self.model = CHAT_MODEL

    def generate_answer(self, question: str, context: str) -> str:
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸‰å›½çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹å‚è€ƒæ–‡æ¡£å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

å‚è€ƒæ–‡æ¡£ï¼š
{context}

é—®é¢˜ï¼š{question}

è¦æ±‚ï¼š
1. ç­”æ¡ˆå¿…é¡»åŸºäºå‚è€ƒæ–‡æ¡£ä¸­çš„ä¿¡æ¯
2. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜"æä¾›çš„æ–‡æ¡£ä¸­æ²¡æœ‰åŒ…å«è¯¥é—®é¢˜çš„ç­”æ¡ˆ"
3. ä¿æŒç­”æ¡ˆç®€æ´å‡†ç¡®

ç­”æ¡ˆï¼š"""

        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {"temperature": 0.3, "num_predict": 1000}
                },
                timeout=120
            )

            if response.status_code == 200:
                return response.json().get("response", "âš ï¸  æ— æ³•ç”Ÿæˆç­”æ¡ˆ")
            return f"âš ï¸  ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {response.status_code}"
        except Exception as e:
            return f"âš ï¸  ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {e}"

class RAGSystemWithRerank:
    """å¸¦ Rerank çš„ RAG ç³»ç»Ÿ"""
    def __init__(self, rerank_method: str = "vector"):
        self.embedder = OllamaEmbedding()
        self.chat = OllamaChat()
        self.index = FaissIndex()
        self.reranker = Reranker(method=rerank_method)

    def build_knowledge_base(self):
        """æ„å»ºçŸ¥è¯†åº“"""
        print("\n" + "=" * 80)
        print("ğŸ“š ä¸‰å›½çŸ¥è¯†åº“æ„å»º")
        print("=" * 80)

        # åŠ è½½æ–‡æ¡£
        print("\nç¬¬ä¸€æ­¥ï¼šåŠ è½½æ–‡æ¡£")
        loader = DocumentLoader()
        documents = loader.load_directory(DOCS_DIR)

        if not documents:
            print(f"âŒ æœªæ‰¾åˆ°æ–‡æ¡£ï¼Œè¯·åœ¨ {DOCS_DIR}/ ç›®å½•ä¸­æ”¾å…¥ä¸‰å›½æ–‡æ¡£")
            return

        print(f"\nâœ… å…±åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")

        # åˆ†å‰²æ–‡æœ¬
        print("\nç¬¬äºŒæ­¥ï¼šåˆ†å‰²æ–‡æœ¬")
        splitter = TextSplitter()
        chunks = splitter.split_documents(documents)
        print(f"âœ… å…±åˆ†å‰²æˆ {len(chunks)} ä¸ªæ–‡æœ¬å—")

        # ç”Ÿæˆ embedding
        print("\nç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆ Embedding")
        texts = [chunk['content'] for chunk in chunks]
        embeddings = self.embedder.get_embeddings_batch(texts)

        # æ„å»ºç´¢å¼•
        print("\nç¬¬å››æ­¥ï¼šæ„å»º Faiss ç´¢å¼•")
        self.index.build_index(chunks, embeddings)

        # ä¿å­˜
        print("\nç¬¬äº”æ­¥ï¼šä¿å­˜ç´¢å¼•")
        self.index.save()
        print("\nâœ¨ ä¸‰å›½çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼")

    def load_knowledge_base(self):
        """åŠ è½½çŸ¥è¯†åº“"""
        self.index.load()
        print("âœ… ä¸‰å›½çŸ¥è¯†åº“å·²åŠ è½½")

    def query(self, question: str) -> Dict:
        """æŸ¥è¯¢"""
        print(f"\n{'=' * 80}")
        print(f"ğŸ” æŸ¥è¯¢: {question}")
        print(f"{'=' * 80}")

        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = self.embedder.get_embedding(question)
        if not query_embedding:
            return {"error": "æ— æ³•ç”ŸæˆæŸ¥è¯¢å‘é‡"}

        # ç²—æ’
        print(f"\nğŸ“Š ç¬¬ä¸€æ­¥ï¼šç²—æ’ï¼ˆå‘é‡æ£€ç´¢ï¼Œå¬å› top-{COARSE_TOP_K}ï¼‰")
        coarse_results = self.index.search(query_embedding, top_k=COARSE_TOP_K)
        print(f"âœ… ç²—æ’å®Œæˆ")

        # ç²¾æ’
        print(f"\nğŸ¯ ç¬¬äºŒæ­¥ï¼šç²¾æ’ï¼ˆRerankï¼‰")
        reranked_results = self.reranker.rerank(question, query_embedding, coarse_results)
        print(f"âœ… ç²¾æ’å®Œæˆ")

        # æ˜¾ç¤ºå¯¹æ¯”
        print(f"\nğŸ“Š ç²—æ’ vs ç²¾æ’å¯¹æ¯”ï¼ˆTop-5ï¼‰:")
        print("\n" + "â”€" * 80)
        for i in range(5):
            coarse = coarse_results[i]
            rerank = reranked_results[i]
            print(f"\n  [{i+1}] ç²—æ’: {coarse['metadata'][:30]:30s} (L2: {coarse['coarse_score']:.4f})")
            print(f"      ç²¾æ’: {rerank['metadata'][:30]:30s} (rerank: {rerank.get('rerank_score', 0):.4f})")

            # å¦‚æœæ’åå˜åŒ–ï¼Œæ ‡å‡ºæ¥
            if coarse['metadata'] != rerank['metadata']:
                print(f"      âš ï¸  æ’åå˜åŒ–ï¼")

        # æœ€ç»ˆç»“æœ
        final_results = reranked_results[:FINAL_TOP_K]

        # ç»„è£…ä¸Šä¸‹æ–‡
        context = "\n\n".join([
            f"ã€æ¥æº: {r['metadata'][:50]}ã€‘\n{r['content'][:300]}..."
            for r in final_results
        ])

        # ç”Ÿæˆç­”æ¡ˆ
        print(f"\nğŸ’­ æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...")
        answer = self.chat.generate_answer(question, context)

        return {
            "answer": answer,
            "sources": final_results,
            "coarse_results": coarse_results[:FINAL_TOP_K],
            "query": question
        }

# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================

def main():
    print("=" * 80)
    print("ğŸ¤– ä¸‰å›½çŸ¥è¯†åº“æ£€ç´¢ç³»ç»Ÿï¼ˆOllama + Faiss + Rerankï¼‰")
    print("=" * 80)
    print("\nğŸ“š åŸºäºã€Šä¸‰å›½æ¼”ä¹‰ã€‹1.7MB æ–‡æœ¬")
    print("ğŸ¯ æ¼”ç¤º Rerank åœ¨å¤§æ•°æ®é›†ä¸Šçš„æ•ˆæœ")

    # æ£€æŸ¥ Ollama
    try:
        response = requests.get(f"{OLLAMA_BASE_URL}/api/tags", timeout=5)
        if response.status_code != 200:
            print("\nâŒ Ollama æœªè¿è¡Œ")
            return
    except:
        print("\nâŒ æ— æ³•è¿æ¥ Ollama")
        return

    # é€‰æ‹© Rerank æ–¹æ³•
    print("\nè¯·é€‰æ‹© Rerank æ–¹æ³•:")
    print("1. å‘é‡ç›¸ä¼¼åº¦ï¼ˆæœ€å¿«ï¼Œæ¨èï¼‰")
    print("2. å…³é”®è¯åŒ¹é…")
    print("\nè¾“å…¥ 1 æˆ– 2ï¼ˆé»˜è®¤ 1ï¼‰: ", end="")

    try:
        choice = input().strip()
        rerank_method = "vector" if choice != "2" else "keyword"
        print(f"\nâœ… ä½¿ç”¨ {'å‘é‡ç›¸ä¼¼åº¦' if rerank_method == 'vector' else 'å…³é”®è¯åŒ¹é…'} æ–¹æ³•")
    except:
        rerank_method = "vector"
        print("\nâœ… ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦æ–¹æ³•ï¼ˆé»˜è®¤ï¼‰")

    # åˆ›å»ºç³»ç»Ÿ
    try:
        rag = RAGSystemWithRerank(rerank_method=rerank_method)
    except Exception as e:
        print(f"\nâŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        print("\nğŸ’¡ è¯·ç¡®ä¿å·²ä¸‹è½½ embedding æ¨¡å‹: ollama pull nomic-embed-text")
        return

    # æ„å»ºæˆ–åŠ è½½çŸ¥è¯†åº“
    if Path(f"{INDEX_PREFIX}_faiss.bin").exists():
        print(f"\næ£€æµ‹åˆ°å·²æœ‰ç´¢å¼•ï¼Œæ˜¯å¦é‡æ–°æ„å»ºï¼Ÿ(y/n): ", end="")
        try:
            choice = input().strip().lower()
            if choice == 'y':
                rag.build_knowledge_base()
            else:
                rag.load_knowledge_base()
        except:
            rag.load_knowledge_base()
    else:
        rag.build_knowledge_base()

    # äº¤äº’å¼é—®ç­”
    print("\n" + "=" * 80)
    print("ğŸ’¬ å¼€å§‹é—®ç­”ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰")
    print("=" * 80)

    # æç¤ºç¤ºä¾‹é—®é¢˜
    print("\nğŸ’¡ ç¤ºä¾‹é—®é¢˜:")
    print("  - åˆ˜å¤‡çš„ä¸‰ä¸ªå…„å¼Ÿæ˜¯è°ï¼Ÿ")
    print("  - æ›¹æ“æ˜¯å¦‚ä½•èµ·å®¶çš„ï¼Ÿ")
    print("  - æ¡ƒå›­ä¸‰ç»“ä¹‰æ˜¯åœ¨å“ªé‡Œï¼Ÿ")
    print("  - å•å¸ƒçš„æ­¦å™¨æ˜¯ä»€ä¹ˆï¼Ÿ")
    print("  - è‘£å“æ˜¯æ€ä¹ˆæ­»çš„ï¼Ÿ")
    print("  - èµ¤å…”é©¬æ˜¯è°çš„ï¼Ÿ")

    while True:
        print("\n" + "â”€" * 80)
        try:
            question = input("â“ ä½ çš„é—®é¢˜: ").strip()

            if not question:
                continue

            if question.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ å†è§ï¼")
                break

            result = rag.query(question)

            if "error" in result:
                print(f"\nâŒ {result['error']}")
                continue

            print("\n" + "â”€" * 80)
            print("ğŸ“– ç­”æ¡ˆ:")
            print("â”€" * 80)
            print(result['answer'])
            print("â”€" * 80)

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ å‡ºé”™äº†: {e}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    main()
