"""
æƒ…æ„Ÿåˆ†æå®æˆ˜ï¼šEmbedding + é€»è¾‘å›å½’
ç»“åˆæ‰€å­¦çŸ¥è¯†ï¼Œå®Œæˆä¸€ä¸ªå®Œæ•´çš„NLPé¡¹ç›®
"""

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import jieba
from gensim.models import Word2Vec
import matplotlib.pyplot as plt
import seaborn as sns

plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False

print("="*70)
print("æƒ…æ„Ÿåˆ†æå®æˆ˜ï¼šEmbedding + é€»è¾‘å›å½’")
print("="*70)

# ===== ç¬¬ä¸€éƒ¨åˆ†ï¼šå‡†å¤‡æ•°æ® =====
print("\n" + "="*70)
print("ç¬¬ä¸€éƒ¨åˆ†ï¼šå‡†å¤‡æƒ…æ„Ÿåˆ†ææ•°æ®é›†")
print("="*70)

# ç¤ºä¾‹æ•°æ®ï¼ˆå®é™…åº”è¯¥ä»æ–‡ä»¶è¯»å–ï¼‰
data = [
    # æ­£é¢è¯„ä»·
    ("è¿™éƒ¨ç”µå½±å¤ªå¥½çœ‹äº†ï¼Œæ¼”å‘˜æ¼”æŠ€å¾ˆæ£’ï¼", 1),
    ("éå¸¸ç²¾å½©çš„å‰§æƒ…ï¼Œå¼ºçƒˆæ¨èï¼", 1),
    ("å¯¼æ¼”åŠŸåŠ›æ·±åšï¼Œè§†è§‰æ•ˆæœéœ‡æ’¼", 1),
    ("äº”æ˜Ÿå¥½è¯„ï¼Œä¸€å®šè¦çœ‹ï¼", 1),
    ("å¤ªå¥½ç¬‘äº†ï¼Œå…¨ç¨‹æ— å°¿ç‚¹", 1),
    ("å‰§æƒ…ç´§å‡‘ï¼Œæ‚¬å¿µè¿­èµ·", 1),
    ("æ¼”å‘˜é¢œå€¼åœ¨çº¿ï¼Œæ¼”æŠ€åœ¨çº¿", 1),
    ("ä»Šå¹´æœ€å¥½çš„ç”µå½±ï¼Œæ²¡æœ‰ä¹‹ä¸€", 1),
    ("æ„Ÿäººè‡³æ·±ï¼Œå€¼å¾—ä¸€çœ‹", 1),
    ("ç‰¹æ•ˆåˆ¶ä½œç²¾è‰¯ï¼ŒéŸ³æ•ˆéœ‡æ’¼", 1),
    ("æ•…äº‹æƒ…èŠ‚è·Œå®•èµ·ä¼", 1),
    ("æ¼”å‘˜è¡¨æ¼”è‡ªç„¶çœŸå®", 1),
    ("å‰§æœ¬å†™å¾—å¾ˆå¥½", 1),
    ("å…¨ç¨‹é«˜èƒ½ï¼Œä¸å®¹é”™è¿‡", 1),
    ("ç»å¯¹çš„ç¥ä½œ", 1),

    # è´Ÿé¢è¯„ä»·
    ("å‰§æƒ…å¤ªæ— èŠäº†ï¼Œçœ‹äº†åŠå°æ—¶å°±ç¡ç€äº†", 0),
    ("æµªè´¹æ—¶é—´å’Œé‡‘é’±ï¼Œä¸æ¨è", 0),
    ("æ¼”å‘˜æ¼”æŠ€å°´å°¬", 0),
    ("å‰§æƒ…é€»è¾‘ä¸é€šï¼Œè«åå…¶å¦™", 0),
    ("ç‰¹æ•ˆå¤ªå‡ï¼Œå»‰ä»·æ„Ÿåè¶³", 0),
    ("å¯¼æ¼”æ‹çš„æ˜¯ä»€ä¹ˆä¸œè¥¿", 0),
    ("å…¨ç¨‹ç©æ‰‹æœºï¼Œå¤ªæ— èŠäº†", 0),
    ("åæ‚”æ¥çœ‹è¿™éƒ¨ç”µå½±", 0),
    ("ä¸€æ— æ˜¯å¤„ï¼Œå¼ºçƒˆå·®è¯„", 0),
    ("å‰§æœ¬çƒ‚ï¼Œæ¼”å‘˜çƒ‚ï¼Œå¯¼æ¼”çƒ‚", 0),
    ("æƒ…èŠ‚æ‹–æ²“ï¼ŒèŠ‚å¥æ··ä¹±", 0),
    ("çœ‹å®Œæƒ³é€€ç¥¨", 0),
    ("æµªè´¹æ—¶é—´ï¼Œæ¯«æ— è¥å…»", 0),
    ("æ¼”æŠ€æµ®å¤¸ï¼Œå°è¯å°´å°¬", 0),
    ("å®Œå…¨ä¸å€¼ç¥¨ä»·", 0),
]

print(f"\næ•°æ®é›†å¤§å°: {len(data)} æ¡è¯„è®º")
print(f"æ­£é¢è¯„ä»·: {sum(1 for _, label in data if label == 1)} æ¡")
print(f"è´Ÿé¢è¯„ä»·: {sum(1 for _, label in data if label == 0)} æ¡")

print("\nç¤ºä¾‹æ•°æ®ï¼š")
print("  æ­£é¢:", data[0][0])
print("  è´Ÿé¢:", data[15][0])

# ===== ç¬¬äºŒéƒ¨åˆ†ï¼šç‰¹å¾æå– - ä¸‰ç§æ–¹æ³•å¯¹æ¯” =====
print("\n" + "="*70)
print("ç¬¬äºŒéƒ¨åˆ†ï¼šç‰¹å¾æå–æ–¹æ³•å¯¹æ¯”")
print("="*70)

# å…ˆç”¨ç®€å•æ•°æ®è®­ç»ƒä¸€ä¸ªå°å‹ Word2Vec
print("\næ­¥éª¤1: è®­ç»ƒ Word2Vec æ¨¡å‹ï¼ˆç”¨äºè¯åµŒå…¥ï¼‰")
all_texts = [text for text, _ in data]
tokenized_texts = [list(jieba.cut(text)) for text in all_texts]

w2v_model = Word2Vec(
    sentences=tokenized_texts,
    vector_size=50,
    window=3,
    min_count=1,
    sg=0,
    epochs=100,
    seed=42
)

print(f"âœ“ Word2Vec è®­ç»ƒå®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°: {len(w2v_model.wv)}")

print("\n" + "-"*70)
print("æ–¹æ³•1: One-Hot + è¯é¢‘ç»Ÿè®¡")
print("-"*70)
print("åŸç†: ç»Ÿè®¡æ¯ä¸ªè¯å‡ºç°çš„æ¬¡æ•°")
print("ç¼ºç‚¹: å¿½ç•¥è¯åºã€æ— æ³•è¡¨ç¤ºè¯­ä¹‰")

print("\n" + "-"*70)
print("æ–¹æ³•2: TF-IDF")
print("-"*70)
print("åŸç†: è¯é¢‘-é€†æ–‡æ¡£é¢‘ç‡")
print("ç¼ºç‚¹: ä»ç„¶å¿½ç•¥è¯åºã€è¯­ä¹‰")

print("\n" + "-"*70)
print("æ–¹æ³•3: Word Embeddingï¼ˆè¯å‘é‡å¹³å‡ï¼‰âœ“ æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ª")
print("-"*70)
print("åŸç†: å°†æ¯ä¸ªè¯è½¬æ¢ä¸ºå‘é‡ï¼Œç„¶åå¹³å‡")
print("ä¼˜ç‚¹: èƒ½æ•æ‰è¯­ä¹‰ä¿¡æ¯")

# æ–¹æ³•3çš„å®ç°
def text_to_embedding(text, model):
    """
    å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯å‘é‡çš„å¹³å‡å€¼
    """
    words = list(jieba.cut(text))
    vectors = [model.wv[word] for word in words if word in model.wv]

    if len(vectors) == 0:
        # å¦‚æœæ²¡æœ‰è¯åœ¨è¯è¡¨ä¸­ï¼Œè¿”å›é›¶å‘é‡
        return np.zeros(model.vector_size)

    # å¹³å‡æ‰€æœ‰è¯å‘é‡
    return np.mean(vectors, axis=0)

# ç¤ºä¾‹
sample_text = "è¿™éƒ¨ç”µå½±å¾ˆå¥½çœ‹"
embedding = text_to_embedding(sample_text, w2v_model)
print(f"\nç¤ºä¾‹: '{sample_text}'")
print(f"  åˆ†è¯: {list(jieba.cut(sample_text))}")
print(f"  å‘é‡ç»´åº¦: {len(embedding)}")
print(f"  å‘é‡å‰5ç»´: {embedding[:5]}")

# ===== ç¬¬ä¸‰éƒ¨åˆ†ï¼šå‡†å¤‡è®­ç»ƒæ•°æ® =====
print("\n" + "="*70)
print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šå‡†å¤‡è®­ç»ƒæ•°æ®")
print("="*70)

# æå–ç‰¹å¾å’Œæ ‡ç­¾
X = np.array([text_to_embedding(text, w2v_model) for text, _ in data])
y = np.array([label for _, label in data])

print(f"\nç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X.shape}")
print(f"  {X.shape[0]} ä¸ªæ ·æœ¬")
print(f"  æ¯ä¸ªæ ·æœ¬ {X.shape[1]} ç»´")

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"\næ•°æ®é›†åˆ’åˆ†:")
print(f"  è®­ç»ƒé›†: {len(X_train)} æ¡")
print(f"  æµ‹è¯•é›†: {len(X_test)} æ¡")

# ===== ç¬¬å››éƒ¨åˆ†ï¼šè®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹ =====
print("\n" + "="*70)
print("ç¬¬å››éƒ¨åˆ†ï¼šè®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹")
print("="*70)

print("\næ¨¡å‹å‚æ•°:")
print("  ç®—æ³•: LogisticRegression")
print("  æ­£åˆ™åŒ–: L2 (C=1.0)")
print("  æ±‚è§£å™¨: lbfgs")

# è®­ç»ƒæ¨¡å‹
print("\nå¼€å§‹è®­ç»ƒ...")
model = LogisticRegression(C=1.0, max_iter=1000, random_state=42)
model.fit(X_train, y_train)
print("âœ“ è®­ç»ƒå®Œæˆï¼")

# æŸ¥çœ‹æ¨¡å‹å‚æ•°
print(f"\næ¨¡å‹æƒé‡å½¢çŠ¶: {model.coef_.shape}")
print(f"æ¨¡å‹æˆªè·: {model.intercept_[0]:.4f}")

# ===== ç¬¬äº”éƒ¨åˆ†ï¼šæ¨¡å‹è¯„ä¼° =====
print("\n" + "="*70)
print("ç¬¬äº”éƒ¨åˆ†ï¼šæ¨¡å‹è¯„ä¼°")
print("="*70)

# è®­ç»ƒé›†è¯„ä¼°
y_train_pred = model.predict(X_train)
train_acc = accuracy_score(y_train, y_train_pred)

# æµ‹è¯•é›†è¯„ä¼°
y_test_pred = model.predict(X_test)
test_acc = accuracy_score(y_test, y_test_pred)

print(f"\nå‡†ç¡®ç‡:")
print(f"  è®­ç»ƒé›†: {train_acc:.2%}")
print(f"  æµ‹è¯•é›†: {test_acc:.2%}")

# è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
print("\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
print("-"*70)
print(classification_report(y_test, y_test_pred, target_names=["è´Ÿé¢", "æ­£é¢"]))

# æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y_test, y_test_pred)
print("\næ··æ·†çŸ©é˜µ:")
print("-"*70)
print("           é¢„æµ‹è´Ÿé¢  é¢„æµ‹æ­£é¢")
print(f"çœŸå®è´Ÿé¢:    {cm[0][0]:>2}       {cm[0][1]:>2}")
print(f"çœŸå®æ­£é¢:    {cm[1][0]:>2}       {cm[1][1]:>2}")

# å¯è§†åŒ–æ··æ·†çŸ©é˜µ
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=["è´Ÿé¢", "æ­£é¢"],
            yticklabels=["è´Ÿé¢", "æ­£é¢"])
plt.title('æ··æ·†çŸ©é˜µ - æƒ…æ„Ÿåˆ†ç±»', fontsize=14, fontweight='bold')
plt.ylabel('çœŸå®æ ‡ç­¾', fontsize=12)
plt.xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12)
plt.tight_layout()
plt.savefig('sentiment_confusion_matrix.png', dpi=100, bbox_inches='tight')
print("\nâœ“ æ··æ·†çŸ©é˜µå›¾å·²ä¿å­˜: sentiment_confusion_matrix.png")
plt.close()

# ===== ç¬¬å…­éƒ¨åˆ†ï¼šé¢„æµ‹æ–°è¯„è®º =====
print("\n" + "="*70)
print("ç¬¬å…­éƒ¨åˆ†ï¼šé¢„æµ‹æ–°è¯„è®º")
print("="*70)

new_reviews = [
    "è¿™éƒ¨ç”µå½±éå¸¸ç²¾å½©ï¼Œå€¼å¾—æ¨èï¼",
    "å¤ªæ— èŠäº†ï¼Œåæ‚”æ¥çœ‹",
    "æ¼”å‘˜æ¼”æŠ€å¾ˆæ£’ï¼Œå‰§æƒ…ä¹Ÿå¾ˆå¥½",
    "å‰§æƒ…æ··ä¹±ï¼Œæµªè´¹æ—¶é—´",
    "äº”æ˜Ÿå¥½è¯„ï¼Œå¼ºçƒˆæ¨èï¼",
    "å®Œå…¨ä¸æ¨èï¼Œå¤ªçƒ‚äº†",
]

print("\næµ‹è¯•è¯„è®º:")
print("-"*70)

for i, review in enumerate(new_reviews, 1):
    # è½¬æ¢ä¸ºå‘é‡
    embedding = text_to_embedding(review, w2v_model).reshape(1, -1)

    # é¢„æµ‹
    pred = model.predict(embedding)[0]
    prob = model.predict_proba(embedding)[0]

    sentiment = "ğŸ˜Š æ­£é¢" if pred == 1 else "ğŸ˜ è´Ÿé¢"
    confidence = prob[pred] * 100

    print(f"\n{i}. {review}")
    print(f"   é¢„æµ‹: {sentiment}")
    print(f"   ç½®ä¿¡åº¦: {confidence:.1f}%")
    print(f"   æ¦‚ç‡åˆ†å¸ƒ: è´Ÿé¢ {prob[0]:.1%} | æ­£é¢ {prob[1]:.1%}")

# ===== ç¬¬ä¸ƒéƒ¨åˆ†ï¼šåˆ†æé¢„æµ‹é”™è¯¯ =====
print("\n" + "="*70)
print("ç¬¬ä¸ƒéƒ¨åˆ†ï¼šåˆ†æé¢„æµ‹ç»“æœ")
print("="*70)

# è·å–æ‰€æœ‰é¢„æµ‹æ¦‚ç‡
y_test_prob = model.predict_proba(X_test)[:, 1]

print("\næµ‹è¯•é›†é¢„æµ‹è¯¦æƒ…:")
print("-"*70)
print(f"{'è¯„è®º':<30} {'çœŸå®':<8} {'é¢„æµ‹':<8} {'æ­£ç¡®æ¦‚ç‡'}")
print("-"*70)

for i, (text, true_label) in enumerate([data[idx] for idx in range(len(data)) if idx % 7 == 0][:5]):
    emb = text_to_embedding(text, w2v_model).reshape(1, -1)
    pred = model.predict(emb)[0]
    prob = model.predict_proba(emb)[0][pred]

    true_sentiment = "æ­£é¢" if true_label == 1 else "è´Ÿé¢"
    pred_sentiment = "æ­£é¢" if pred == 1 else "è´Ÿé¢"
    correct = "âœ“" if pred == true_label else "âœ—"

    print(f"{text[:28]:<30} {true_sentiment:<8} {pred_sentiment:<8} {prob:.1%} {correct}")

# ===== ç¬¬å…«éƒ¨åˆ†ï¼šå¯è§†åŒ–è¯å‘é‡ =====
print("\n" + "="*70)
print("ç¬¬å…«éƒ¨åˆ†ï¼šå¯è§†åŒ–æƒ…æ„Ÿè¯å‘é‡")
print("="*70)

from sklearn.decomposition import PCA

# é€‰æ‹©ä¸€äº›æƒ…æ„Ÿè¯
positive_words = ["å¥½çœ‹", "ç²¾å½©", "æ¨è", "æ£’", "å¥½", "ä¼˜ç§€"]
negative_words = ["æ— èŠ", "çƒ‚", "å·®", "åƒåœ¾", "åæ‚”", "å·®è¯„"]

# æå–è¯å‘é‡
all_words = positive_words + negative_words
vectors = []
labels = []
colors = []

for word in positive_words:
    if word in w2v_model.wv:
        vectors.append(w2v_model.wv[word])
        labels.append(word)
        colors.append('green')

for word in negative_words:
    if word in w2v_model.wv:
        vectors.append(w2v_model.wv[word])
        labels.append(word)
        colors.append('red')

vectors = np.array(vectors)

# PCAé™ç»´
pca = PCA(n_components=2)
vectors_2d = pca.fit_transform(vectors)

# å¯è§†åŒ–
plt.figure(figsize=(10, 8))
for i, (x, y) in enumerate(vectors_2d):
    plt.scatter(x, y, c=colors[i], s=200, alpha=0.6,
               edgecolors='black', linewidth=1.5)
    plt.annotate(labels[i], (x, y), xytext=(5, 5),
                textcoords='offset points', fontsize=12, fontweight='bold')

# æ·»åŠ å›¾ä¾‹
from matplotlib.lines import Line2D
legend_elements = [
    Line2D([0], [0], marker='o', color='w', markerfacecolor='green',
           markersize=12, label='æ­£é¢è¯'),
    Line2D([0], [0], marker='o', color='w', markerfacecolor='red',
           markersize=12, label='è´Ÿé¢è¯'),
]
plt.legend(handles=legend_elements, loc='best', fontsize=11)

plt.title('æƒ…æ„Ÿè¯å‘é‡å¯è§†åŒ– (PCAé™ç»´)', fontsize=14, fontweight='bold')
plt.xlabel('ç»´åº¦ 1 (PCA)', fontsize=12)
plt.ylabel('ç»´åº¦ 2 (PCA)', fontsize=12)
plt.grid(True, alpha=0.3)
plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)
plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)

plt.tight_layout()
plt.savefig('sentiment_word_vectors.png', dpi=100, bbox_inches='tight')
print("âœ“ æƒ…æ„Ÿè¯å‘é‡å›¾å·²ä¿å­˜: sentiment_word_vectors.png")
plt.close()

# ===== ç¬¬ä¹éƒ¨åˆ†ï¼šæ€»ç»“ä¸æ”¹è¿› =====
print("\n" + "="*70)
print("æ€»ç»“ä¸æ”¹è¿›å»ºè®®")
print("="*70)

print("\nâœ“ æˆ‘ä»¬å®Œæˆäº†ä»€ä¹ˆ:")
print("  1. ä½¿ç”¨ Word2Vec å°†è¯è½¬æ¢ä¸ºå‘é‡")
print("  2. ç”¨è¯å‘é‡å¹³å‡è¡¨ç¤ºå¥å­")
print("  3. è®­ç»ƒé€»è¾‘å›å½’åˆ†ç±»å™¨")
print("  4. é¢„æµ‹æ–°è¯„è®ºçš„æƒ…æ„Ÿ")

print("\nğŸ“Š æ¨¡å‹æ€§èƒ½:")
print(f"  æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.2%}")
print("  æ³¨: ç”±äºæ•°æ®é›†å¾ˆå°ï¼ˆä»…30æ¡ï¼‰ï¼Œå®é™…æ€§èƒ½ä¸ä»£è¡¨çœŸå®æ°´å¹³")

print("\nğŸ”§ æ”¹è¿›æ–¹å‘:")
print("\n1. æ•°æ®å±‚é¢:")
print("   - ä½¿ç”¨æ›´å¤§çš„æ•°æ®é›†ï¼ˆè‡³å°‘å‡ åƒæ¡ï¼‰")
print("   - æ•°æ®æ¸…æ´—ï¼šå»é™¤æ ‡ç‚¹ã€åœç”¨è¯")
print("   - æ•°æ®å¢å¼ºï¼šåŒä¹‰è¯æ›¿æ¢ã€å›è¯‘")

print("\n2. ç‰¹å¾å·¥ç¨‹:")
print("   - ä½¿ç”¨é¢„è®­ç»ƒè¯å‘é‡ï¼ˆè…¾è®¯ã€åŒ—å¸ˆå¤§ï¼‰")
print("   - å°è¯•ä¸åŒçš„å¥å­è¡¨ç¤ºæ–¹æ³•")
print("   - æ·»åŠ TF-IDFåŠ æƒ")

print("\n3. æ¨¡å‹ä¼˜åŒ–:")
print("   - è°ƒæ•´æ­£åˆ™åŒ–å‚æ•° C")
print("   - å°è¯•å…¶ä»–ç®—æ³•ï¼ˆSVMã€éšæœºæ£®æ—ï¼‰")
print("   - ä½¿ç”¨æ·±åº¦å­¦ä¹ ï¼ˆLSTMã€BERTï¼‰")

print("\n4. é«˜çº§æŠ€æœ¯:")
print("   - Sentence-BERTï¼ˆå¥å­åµŒå…¥ï¼‰")
print("   - æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttentionï¼‰")
print("   - é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆBERTã€GPTï¼‰")

print("\n" + "="*70)
print("ä¸‹ä¸€æ­¥å­¦ä¹ ")
print("="*70)

print("\næ¨èé¡¹ç›®:")
print("  1. ä½¿ç”¨çœŸå®çš„ç”µå½±è¯„è®ºæ•°æ®é›†ï¼ˆIMDBã€è±†ç“£ï¼‰")
print("  2. å°è¯•å¤šåˆ†ç±»é—®é¢˜ï¼ˆ1-5æ˜Ÿè¯„åˆ†ï¼‰")
print("  3. å­¦ä¹ æ›´å¤æ‚çš„æ¨¡å‹ï¼ˆLSTMã€Transformerï¼‰")

print("\næ¨èèµ„æº:")
print("  - IMDB ç”µå½±è¯„è®ºæ•°æ®é›†")
print("  - è±†ç“£ç”µå½±è¯„è®ºï¼ˆçˆ¬è™«è·å–ï¼‰")
print("  - ä¸­æ–‡æƒ…æ„Ÿåˆ†ææ•°æ®é›†ï¼ˆGitHubæœç´¢ï¼‰")

print("\n" + "="*70)
print("æ¼”ç¤ºå®Œæˆï¼")
print("="*70)

print("\nğŸ’¡ æç¤º:")
print("  è¿™ä¸ªé¡¹ç›®æ•´åˆäº†ä¸¤ä¸ªé‡è¦æ¦‚å¿µ:")
print("  1. Embedding - å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—å‘é‡")
print("  2. é€»è¾‘å›å½’ - åˆ†ç±»ç®—æ³•")
print("  ç»“åˆèµ·æ¥å°±èƒ½å®Œæˆæƒ…æ„Ÿåˆ†æä»»åŠ¡ï¼")
