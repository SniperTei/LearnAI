"""
Word2Vec å®æˆ˜æ¼”ç¤º - ä¸­æ–‡è¯å‘é‡è®­ç»ƒä¸ä½¿ç”¨
è¾¹å­¦è¾¹ç»ƒï¼šä»åŸç†åˆ°å®æˆ˜
"""

import numpy as np
from gensim.models import Word2Vec
import jieba
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt

plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False

print("="*70)
print("Word2Vec å®æˆ˜æ¼”ç¤ºï¼šè®©æœºå™¨ç†è§£è¯è¯­çš„å«ä¹‰")
print("="*70)

# ===== ç¬¬ä¸€éƒ¨åˆ†ï¼šå‡†å¤‡ä¸­æ–‡è¯­æ–™ =====
print("\n" + "="*70)
print("ç¬¬ä¸€éƒ¨åˆ†ï¼šå‡†å¤‡ä¸­æ–‡è¯­æ–™")
print("="*70)

# å°å‹ä¸­æ–‡è¯­æ–™ï¼ˆç”¨äºæ¼”ç¤ºï¼‰
corpus = [
    ["æˆ‘", "çˆ±", "åŒ—äº¬", "å¤©å®‰é—¨"],
    ["åŒ—äº¬", "æ˜¯", "ä¸­å›½", "çš„", "é¦–éƒ½"],
    ["ä¸­å›½", "æœ‰", "äº”åƒå¹´", "çš„", "å†å²"],
    ["é•¿åŸ", "æ˜¯", "ä¸­å›½", "çš„", "è±¡å¾"],
    ["æˆ‘", "å–œæ¬¢", "åƒ", "åŒ—äº¬", "çƒ¤é¸­"],
    ["åŒ—äº¬çƒ¤é¸­", "éå¸¸", "å¥½åƒ"],
    ["é•¿åŸ", "ä½äº", "åŒ—äº¬", "éƒŠåŒº"],
    ["å¤©å®‰é—¨", "å¹¿åœº", "å¾ˆ", "å£®è§‚"],
    ["æˆ‘", "å»", "è¿‡", "åŒ—äº¬", "å¾ˆå¤š", "æ¬¡"],
    ["åŒ—äº¬", "çš„", "ç§‹å¤©", "å¾ˆ", "ç¾"],
    ["ä¸­å›½", "çš„", "ç»æµ", "å‘å±•", "å¾ˆå¿«"],
    ["é•¿åŸ", "æ˜¯", "ä¸–ç•Œ", "æ–‡åŒ–é—äº§"],
]

print("\nè¯­æ–™ç¤ºä¾‹ï¼ˆå‰3å¥ï¼‰ï¼š")
for i, sent in enumerate(corpus[:3], 1):
    print(f"  {i}. {' '.join(sent)}")

print(f"\næ€»å¥å­æ•°: {len(corpus)}")
print(f"æ€»è¯æ±‡æ•°: {sum(len(s) for s in corpus)}")

# ===== ç¬¬äºŒéƒ¨åˆ†ï¼šè®­ç»ƒ Word2Vec æ¨¡å‹ =====
print("\n" + "="*70)
print("ç¬¬äºŒéƒ¨åˆ†ï¼šè®­ç»ƒ Word2Vec æ¨¡å‹")
print("="*70)

print("\nWord2Vec å‚æ•°è¯´æ˜ï¼š")
print("  - vector_size: è¯å‘é‡ç»´åº¦ï¼ˆé»˜è®¤100ï¼‰")
print("  - window: ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼ˆé»˜è®¤5ï¼‰")
print("  - min_count: æœ€å°è¯é¢‘ï¼Œä½äºæ­¤å€¼å¿½ç•¥ï¼ˆé»˜è®¤5ï¼‰")
print("  - sg: 1=Skip-gram, 0=CBOWï¼ˆé»˜è®¤0ï¼‰")
print("  - epochs: è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤5ï¼‰")

# è®­ç»ƒæ¨¡å‹
print("\nå¼€å§‹è®­ç»ƒ...")
model = Word2Vec(
    sentences=corpus,
    vector_size=50,      # è¯å‘é‡ç»´åº¦
    window=3,            # ä¸Šä¸‹æ–‡çª—å£
    min_count=1,         # æœ€å°è¯é¢‘ï¼ˆæ¼”ç¤ºç”¨ï¼Œè®¾ä¸º1ï¼‰
    sg=0,                # 0=CBOW, 1=Skip-gram
    epochs=100,          # è®­ç»ƒè½®æ•°ï¼ˆæ¼”ç¤ºç”¨ï¼Œå¢åŠ è½®æ•°ï¼‰
    seed=42
)

print("âœ“ è®­ç»ƒå®Œæˆï¼")
print(f"âœ“ è¯æ±‡è¡¨å¤§å°: {len(model.wv)} ä¸ªè¯")

# æŸ¥çœ‹è¯å‘é‡
print("\n" + "-"*70)
print("è¯å‘é‡ç¤ºä¾‹ï¼ˆå‰5ä¸ªè¯ï¼‰ï¼š")
print("-"*70)

words = list(model.wv.key_to_index.keys())[:5]
print(f"\n{'è¯è¯­':<8} {'è¯å‘é‡ï¼ˆå‰5ç»´ï¼‰'}")
print("-"*70)

for word in words:
    vector = model.wv[word]
    vector_str = ", ".join([f"{v:.2f}" for v in vector[:5]])
    print(f"{word:<8} [{vector_str}, ...]")

# ===== ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ¢ç´¢è¯å‘é‡ =====
print("\n" + "="*70)
print("ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ¢ç´¢è¯å‘é‡")
print("="*70)

# 1. è®¡ç®—è¯ç›¸ä¼¼åº¦
print("\n" + "-"*70)
print("1. è®¡ç®—è¯è¯­ç›¸ä¼¼åº¦")
print("-"*70)

word_pairs = [
    ("åŒ—äº¬", "ä¸­å›½"),
    ("åŒ—äº¬", "é•¿åŸ"),
    ("åŒ—äº¬", "å¤©å®‰é—¨"),
    ("åŒ—äº¬", "æˆ‘"),
    ("é•¿åŸ", "ä¸­å›½"),
    ("æˆ‘", "å–œæ¬¢"),
]

for word1, word2 in word_pairs:
    if word1 in model.wv and word2 in model.wv:
        sim = model.wv.similarity(word1, word2)
        bar = "â–ˆ" * int(sim * 30)
        print(f"  '{word1}' vs '{word2}': {sim:.3f} {bar}")

# 2. æ‰¾æœ€ç›¸ä¼¼çš„è¯
print("\n" + "-"*70)
print("2. æ‰¾æœ€ç›¸ä¼¼çš„è¯")
print("-"*70)

query_words = ["åŒ—äº¬", "é•¿åŸ", "ä¸­å›½", "åƒ"]

for word in query_words:
    if word in model.wv:
        similar_words = model.wv.most_similar(word, topn=3)
        print(f"\n  ä¸ '{word}' æœ€ç›¸ä¼¼çš„è¯ï¼š")
        for similar_word, score in similar_words:
            bar = "â–ˆ" * int(score * 30)
            print(f"    {similar_word:<8} {score:.3f} {bar}")

# 3. è¯å‘é‡ç±»æ¯”
print("\n" + "-"*70)
print("3. è¯å‘é‡ç±»æ¯”ï¼ˆç±»æ¯”æ¨ç†ï¼‰")
print("-"*70)

print("\n  ç»å…¸ä¾‹å­ï¼šå›½ç‹ - ç”·äºº + å¥³äºº â‰ˆ ç‹å")
print("  æˆ‘ä»¬çš„è¯­æ–™å¤ªå°ï¼Œå°è¯•ç®€å•ç±»æ¯”ï¼š")

# å°è¯•ç®€å•ç±»æ¯”
print("\n  å°è¯•: åŒ—äº¬ - ä¸­å›½ + é•¿åŸ â‰ˆ ?")
try:
    result = model.wv.most_similar(
        positive=["åŒ—äº¬", "é•¿åŸ"],
        negative=["ä¸­å›½"],
        topn=3
    )
    for word, score in result:
        print(f"    {word:<8} {score:.3f}")
except Exception as e:
    print(f"    è¯­æ–™å¤ªå°ï¼Œæ— æ³•å®Œæˆç±»æ¯” ğŸ˜…")

print("\n  è§£é‡Š: 'åŒ—äº¬' å‡å» 'ä¸­å›½' çš„éƒ¨åˆ†ç‰¹å¾")
print("        åŠ ä¸Š 'é•¿åŸ' çš„ç‰¹å¾")
print("        çœ‹çœ‹ç»“æœæ¥è¿‘ä»€ä¹ˆè¯")

# ===== ç¬¬å››éƒ¨åˆ†ï¼šWord2Vec ä¸¤ç§æ¨¡å¼å¯¹æ¯” =====
print("\n" + "="*70)
print("ç¬¬å››éƒ¨åˆ†ï¼šCBOW vs Skip-gram å¯¹æ¯”")
print("="*70)

print("\nCBOW (Continuous Bag-of-Words):")
print("  - æ ¹æ®å‘¨å›´è¯é¢„æµ‹ä¸­å¿ƒè¯")
print("  - ä¾‹: 'ä»Šå¤©å¤©æ°”_ä¸é”™' â†’ çŒœ'çœŸ'")
print("  - ä¼˜ç‚¹: è®­ç»ƒå¿«ï¼Œé€‚åˆå¸¸è§è¯")
print("  - ç¼ºç‚¹: å¯¹ç”Ÿåƒ»è¯æ•ˆæœå·®")

print("\nSkip-gram:")
print("  - æ ¹æ®ä¸­å¿ƒè¯é¢„æµ‹å‘¨å›´è¯")
print("  - ä¾‹: 'çœŸ' â†’ çŒœ['ä»Šå¤©', 'å¤©æ°”', 'ä¸é”™']")
print("  - ä¼˜ç‚¹: å¯¹ç”Ÿåƒ»è¯æ•ˆæœå¥½ï¼Œèƒ½å­¦åˆ°æ›´å¤šä¿¡æ¯")
print("  - ç¼ºç‚¹: è®­ç»ƒæ…¢")

# å¯¹æ¯”è®­ç»ƒï¼ˆå°å‹æ¼”ç¤ºï¼‰
print("\n" + "-"*70)
print("å¯¹æ¯”è®­ç»ƒï¼ˆç›¸åŒå‚æ•°ï¼Œä¸åŒæ¨¡å¼ï¼‰")
print("-"*70)

# CBOW
model_cbow = Word2Vec(
    sentences=corpus,
    vector_size=50,
    window=3,
    min_count=1,
    sg=0,  # CBOW
    epochs=50,
    seed=42
)

# Skip-gram
model_sg = Word2Vec(
    sentences=corpus,
    vector_size=50,
    window=3,
    min_count=1,
    sg=1,  # Skip-gram
    epochs=50,
    seed=42
)

print("âœ“ CBOW æ¨¡å‹è®­ç»ƒå®Œæˆ")
print("âœ“ Skip-gram æ¨¡å‹è®­ç»ƒå®Œæˆ")

# å¯¹æ¯”ç›¸ä¼¼åº¦
print("\nå¯¹æ¯”è¯ç›¸ä¼¼åº¦ï¼ˆä»¥'åŒ—äº¬'ä¸ºä¾‹ï¼‰ï¼š")
test_words = ["ä¸­å›½", "é•¿åŸ", "å¤©å®‰é—¨"]
print(f"\n{'å¯¹æ¯”è¯':<8} {'CBOWç›¸ä¼¼åº¦':<15} {'Skip-gramç›¸ä¼¼åº¦'}")
print("-"*70)

for word in test_words:
    if word in model_cbow.wv and word in model_sg.wv:
        sim_cbow = model_cbow.wv.similarity("åŒ—äº¬", word)
        sim_sg = model_sg.wv.similarity("åŒ—äº¬", word)
        print(f"{word:<8} {sim_cbow:<15.3f} {sim_sg:.3f}")

print("\næ³¨æ„: ç”±äºè¯­æ–™å¾ˆå°ï¼Œå·®å¼‚å¯èƒ½ä¸æ˜æ˜¾")
print("å®é™…åº”ç”¨ä¸­ï¼ŒSkip-gram é€šå¸¸å¯¹ç”Ÿåƒ»è¯æ•ˆæœæ›´å¥½")

# ===== ç¬¬äº”éƒ¨åˆ†ï¼šè¯å‘é‡å¯è§†åŒ– =====
print("\n" + "="*70)
print("ç¬¬äº”éƒ¨åˆ†ï¼šè¯å‘é‡å¯è§†åŒ–ï¼ˆé™ç»´åˆ°2Dï¼‰")
print("="*70)

from sklearn.decomposition import PCA

# è·å–æ‰€æœ‰è¯å‘é‡
words = list(model.wv.key_to_index.keys())
vectors = np.array([model.wv[w] for w in words])

# PCAé™ç»´åˆ°2ç»´
pca = PCA(n_components=2)
vectors_2d = pca.fit_transform(vectors)

# å¯è§†åŒ–
plt.figure(figsize=(12, 10))

# ç»˜åˆ¶æ•£ç‚¹å›¾
for i, word in enumerate(words):
    x, y = vectors_2d[i]
    plt.scatter(x, y, alpha=0.6, s=100, edgecolors='black', linewidth=0.5)
    plt.annotate(word, (x, y), xytext=(5, 5), textcoords='offset points',
                fontsize=10, alpha=0.8)

plt.title('Word2Vec è¯å‘é‡å¯è§†åŒ– (PCAé™ç»´)', fontsize=14, fontweight='bold')
plt.xlabel('ç»´åº¦ 1 (PCA)', fontsize=12)
plt.ylabel('ç»´åº¦ 2 (PCA)', fontsize=12)
plt.grid(True, alpha=0.3)
plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)
plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)

plt.tight_layout()
plt.savefig('word2vec_visualization.png', dpi=100, bbox_inches='tight')
print("âœ“ å¯è§†åŒ–å›¾å·²ä¿å­˜: word2vec_visualization.png")
plt.close()

# ===== ç¬¬å…­éƒ¨åˆ†ï¼šå®é™…åº”ç”¨ - å¥å­ç›¸ä¼¼åº¦ =====
print("\n" + "="*70)
print("ç¬¬å…­éƒ¨åˆ†ï¼šå®é™…åº”ç”¨ - è®¡ç®—å¥å­ç›¸ä¼¼åº¦")
print("="*70)

sentences = [
    "æˆ‘çˆ±åŒ—äº¬",
    "æˆ‘å–œæ¬¢åŒ—äº¬",
    "åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½",
    "é•¿åŸåœ¨åŒ—äº¬éƒŠåŒº",
]

print("\nç¤ºä¾‹å¥å­ï¼š")
for i, sent in enumerate(sentences, 1):
    print(f"  {i}. {sent}")

# å¥å­å‘é‡åŒ–æ–¹æ³•ï¼šè¯å‘é‡å¹³å‡
def sentence_vector(sentence, model):
    words = list(jieba.cut(sentence))
    vectors = [model.wv[w] for w in words if w in model.wv]
    if len(vectors) == 0:
        return np.zeros(model.vector_size)
    return np.mean(vectors, axis=0)

print("\n" + "-"*70)
print("å¥å­å‘é‡åŒ–æ–¹æ³•ï¼šè¯å‘é‡å¹³å‡")
print("-"*70)

sentence_vectors = [sentence_vector(s, model) for s in sentences]

print("\nå¥å­ç›¸ä¼¼åº¦çŸ©é˜µï¼š")
print("-"*70)

# æ‰“å°è¡¨å¤´
print(f"{'':12s}", end="")
for i in range(len(sentences)):
    print(f"å¥å­{i+1:>6d}", end="")
print()

print("-"*70)

# æ‰“å°ç›¸ä¼¼åº¦çŸ©é˜µ
for i in range(len(sentences)):
    print(f"å¥å­{i+1:<6d}", end="")
    for j in range(len(sentences)):
        if i == j:
            print(f"  1.000", end="")
        else:
            sim = cosine_similarity([sentence_vectors[i]], [sentence_vectors[j]])[0][0]
            print(f"  {sim:.3f}", end="")
    print()

print("\nè§‚å¯Ÿ:")
print("  - å¥å­1å’Œå¥å­2ç›¸ä¼¼åº¦åº”è¯¥è¾ƒé«˜ï¼ˆéƒ½æ˜¯è¡¨è¾¾å–œçˆ±ï¼‰")
print("  - å¥å­3å’Œå¥å­4ç›¸ä¼¼åº¦ä¹Ÿè¾ƒé«˜ï¼ˆéƒ½ä¸åŒ—äº¬ã€ä¸­å›½ã€é•¿åŸç›¸å…³ï¼‰")

# ===== ç¬¬ä¸ƒéƒ¨åˆ†ï¼šæ€»ç»“ä¸å»ºè®® =====
print("\n" + "="*70)
print("æ€»ç»“ï¼šWord2Vec æ ¸å¿ƒè¦ç‚¹")
print("="*70)

print("\n1. æ ¸å¿ƒæ€æƒ³ï¼š")
print("   'ä¸€ä¸ªè¯çš„æ„æ€ç”±å®ƒå‘¨å›´çš„è¯å†³å®š'")
print("   å‡ºç°åœ¨ç›¸ä¼¼è¯­å¢ƒä¸­çš„è¯ï¼Œå‘é‡åº”è¯¥ç›¸ä¼¼")

print("\n2. ä¸¤ç§æ¨¡å¼ï¼š")
print("   CBOW: å¿«ã€é€‚åˆå¸¸è§è¯")
print("   Skip-gram: æ…¢ã€å¯¹ç”Ÿåƒ»è¯æ•ˆæœå¥½")

print("\n3. å‚æ•°è°ƒä¼˜å»ºè®®ï¼š")
print("   vector_size: å°æ•°æ®50-100ï¼Œå¤§æ•°æ®200-300")
print("   window: ä¸€èˆ¬3-5ï¼Œæ ¹æ®ä»»åŠ¡è°ƒæ•´")
print("   min_count: å¿½ç•¥ä½é¢‘è¯ï¼Œé»˜è®¤5")
print("   epochs: å°æ•°æ®å¤šè½®æ¬¡ï¼Œå¤§æ•°æ®å°‘è½®æ¬¡")

print("\n4. å®é™…åº”ç”¨ï¼š")
print("   - æƒ…æ„Ÿåˆ†æ")
print("   - æ–‡æ¡£ç›¸ä¼¼åº¦")
print("   - æ¨èç³»ç»Ÿ")
print("   - æœºå™¨ç¿»è¯‘")

print("\n5. æ³¨æ„äº‹é¡¹ï¼š")
print("   âš ï¸  æˆ‘ä»¬çš„æ¼”ç¤ºè¯­æ–™å¤ªå°ï¼Œå®é™…éœ€è¦å¤§é‡æ–‡æœ¬ï¼ˆè‡³å°‘ç™¾ä¸‡è¯ï¼‰")
print("   âš ï¸  ä¸­æ–‡éœ€è¦åˆ†è¯ï¼ˆjiebaã€pkusegç­‰ï¼‰")
print("   âš ï¸  å®é™…é¡¹ç›®ä¼˜å…ˆä½¿ç”¨é¢„è®­ç»ƒè¯å‘é‡")

print("\n" + "="*70)
print("ä¸‹ä¸€æ­¥å­¦ä¹ ")
print("="*70)

print("\n1. ä½¿ç”¨æ›´å¤§è§„æ¨¡çš„è¯­æ–™è®­ç»ƒ")
print("2. å°è¯•é¢„è®­ç»ƒè¯å‘é‡ï¼ˆè…¾è®¯ã€åŒ—å¸ˆå¤§ç­‰ï¼‰")
print("3. å­¦ä¹  GloVeã€FastText")
print("4. å®æˆ˜ï¼šæƒ…æ„Ÿåˆ†æã€æ–‡æœ¬åˆ†ç±»")

print("\nè¿è¡Œå‘½ä»¤ï¼š")
print("  python 03_sentiment_analysis.py  # æƒ…æ„Ÿåˆ†æå®æˆ˜")

print("\næ¨èèµ„æºï¼š")
print("  - è…¾è®¯è¯å‘é‡: https://ai.tencent.com/ailab/nlp/embedding.html")
print("  - åŒ—äº¬å¸ˆèŒƒå¤§å­¦ä¸­æ–‡è¯å‘é‡: GitHubæœç´¢ 'Chinese-Word-Vectors'")

print("\n" + "="*70)
print("æ¼”ç¤ºå®Œæˆï¼")
print("="*70)
